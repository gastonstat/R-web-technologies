[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Web Technologies in R",
    "section": "",
    "text": "About\nIn this manuscript I provide an overview of some of the Web Technologies available in R.\n\nAbout You\nI am assuming that you have both R or RStudio installed in your computer. If this is not the case, you can take a look at Breaking the Ice with R\nhttps://www.gastonsanchez.com/R-ice-breaker\n\n\nCitation\nYou can cite this work as:\nSanchez, G. (2022) A Short Introduction to Web Data Technologies in R.  https://www.gastonsanchez.com/R-web-technologies\n\n\n\nMy Series of R Tutorials\nThis document is part of a series of texts that I’ve written about Programming and Data Analysis in R:\n\nBreaking the Ice with R: Getting Started with R and RStudio  https://www.gastonsanchez.com/R-ice-breaker\nTidy Hurricanes: Analyzing Tropical Storms with Tidyverse Tools  https://www.gastonsanchez.com/R-tidy-hurricanes\nR Coding Basics: An Introduction to the Basics of Coding in R  https://www.gastonsanchez.com/R-coding-basics\nRolling Dice: Exploring Simulations in Games of Chance with R  https://www.gastonsanchez.com/R-rolling-dice\nWeb Technologies in R: A Short Introduction to Web Technologies in R  https://www.gastonsanchez.com/R-web-technologies\n\n\n\n\nDonation\nAs a Data Science and Statistics educator, I love to share the work I do. Each month I spend dozens of hours curating learning materials like this resource. If you find any value and usefulness in it, please consider making a one-time donation—via paypal—in any amount (e.g. the amount you would spend inviting me a cup of coffee or any other drink). Your support really matters.\n\n\n\nLicense\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "1-01-introduction.html",
    "href": "1-01-introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "The Web is full of information and resources that can be considered to be sources of data. Statisticians, data analysts, data scientists, researchers, and data-based users in general, increasingly are working in projects that depend on various data sources, many of them either coming or available from the Web.\nAs it turns out, we can use a wide array of approaches to get data from the Web. For instance, we can simply scrape data from human-readable webpages. Likewise, we can also utilize application programming interfaces (APIs) to request some data sets. Interestingly, the data may come in some XML dialect, the most common one being HTML. But it can also come in a JSON document or some other self-describing format. Consequently, you need to be prepared to deal with data from the Web."
  },
  {
    "objectID": "1-01-introduction.html#suggested-tools",
    "href": "1-01-introduction.html#suggested-tools",
    "title": "1  Introduction",
    "section": "1.1 Suggested Tools",
    "text": "1.1 Suggested Tools\nTo enjoy the content of this text, and also to be able to replicate the examples discussed in subsequent chapters, you will need the following tools:\n\nA fairly recent version of R\nA fairly recent version of RStudio\nWeb Browser (e.g. Chrome, Safari, Firefox, Opera)\nand good Internet connection!"
  },
  {
    "objectID": "1-01-introduction.html#suggested-r-packages",
    "href": "1-01-introduction.html#suggested-r-packages",
    "title": "1  Introduction",
    "section": "1.2 Suggested R Packages",
    "text": "1.2 Suggested R Packages\nThe code and examples shown in this book are based on the following packages:\n\n\"tidyverse\" which contains, among other packages:\n\n\"dplyr\": for manipulation of data tables\n\"stringr\": for manipulation of strings and text data\n\n\"xml2\": tools for parsing XML and HTML documents\n\"httr\": tools for working with HTTP requests\n\"rvest\": for harvesting or scraping web data in an easy way\n\"jsonlite\": functions for handling JSON data\n\nBy the way, R has a large collection of packages for interacting with the Web. A comprehensive list of packages for dealing with Web Technologies is available in the following Cran Task View (curated by Mauricio Vargas Sepulveda):\nhttps://cran.r-project.org/web/views/WebTechnologies.html"
  },
  {
    "objectID": "1-01-introduction.html#some-acronyms",
    "href": "1-01-introduction.html#some-acronyms",
    "title": "1  Introduction",
    "section": "1.3 Some Acronyms",
    "text": "1.3 Some Acronyms\nAs you’ll see later in this book, there is a number of acronyms commonly used around all-things Web. I will define and explain every acronym in their corresponding chapter. In the meantime, I would like to give you a first exposure to the following terms:\n\nWWW: World Wide Web\nURL: Uniform Resource Locator\nHTTP: HyperText Transfer Protocol\nXML: Extensible Markup Language\nHTML: HyperText Markup Language\nJSON: JavaScript Object Notation\nAPI: Application Programming Interface"
  },
  {
    "objectID": "1-02-web.html",
    "href": "1-02-web.html",
    "title": "2  The Web",
    "section": "",
    "text": "In this chapter I provide a brief and superficial description of the Web and how it works."
  },
  {
    "objectID": "1-02-web.html#surfing-the-web",
    "href": "1-02-web.html#surfing-the-web",
    "title": "2  The Web",
    "section": "2.1 Surfing the Web",
    "text": "2.1 Surfing the Web\nThink about when you surf the web:\n\nYou open a web browser (e.g. Google Chrome, Safari, Firefox)\nYou type in or click the URL of a website you wish to visit (e.g.  https://www.r-project.org)\nYou wait some fractions of a second, and then the website shows up in your screen.\n\n\n\n\n\n\nSurfing the web\n\n\n\n\n\nWhat exactly is happening “behind the scenes”?\n\nPeople access websites using software called a Web browser (e.g. Google Chrome, Safari, Firefox)\nA browser is a software that, among other things, requests information (e.g. request to access R project’s website)\nUsing more proper language, the browser in your computer is the client that requests a variety of resources (e.g. pages, images, videos, audio, scripts)\nThe client’s request is sent to Web servers\nA server is the software-computer in charge of serving the resources that the clients request.\nThe server sends responses back to the client\n\n\n\n\n\n\nClient makes a request, and the serve responds\n\n\n\n\n\nTo be more accurate, the server is the software that allows the computer to communicate with other computers; however, it is common to use the term “server” to refer to the computer running the software, which also contains other files and programs. Simply put, a server is basically a computer connected to the Internet. The Internet, in turn, is just a network of connected computers forming a system of standards and rules. The purpose of connecting computers together is to share information.\nThe job of the server software is to wait for a request for information, then retrieve and send that information back to the client(s) as fast as possible. In other words, Web servers have a full time job, waiting for requests from Web browsers all over the world."
  },
  {
    "objectID": "1-02-web.html#how-does-the-web-work",
    "href": "1-02-web.html#how-does-the-web-work",
    "title": "2  The Web",
    "section": "2.2 How Does the Web Work?",
    "text": "2.2 How Does the Web Work?\nNow that we have the high level intuition of clients making requests, and servers sending responses back to clients, let’s describe things in more detail.\nTo make web pages, programmers, developers and designers create files written in a special type of syntax called HyperText Markup Language or HTML for short. These files are stored in a Web server.\n\n\n\n\n\nHTML files are the building blocks of web pages\n\n\n\n\n\nTo be more precise, Web servers store more than one single HTML file. In practice, websites are made of several directories containing various types of files (image files, audio files, video files, scripts, etc).\n\n\n\n\n\nWeb server containing several types of files, not just HTML files\n\n\n\n\n\nOnce HTML files are put on the web server, any browser (e.g. Chrome, Safari, Firefox, Explorer) can retrieve the web page over the internet. The browser on your laptop, on your tablet, on your cellphone, you name it. As long as the device you are using is connected to the internet, the browser will retrieve the web page. The HTML content in the web page tells the browser everything it needs to know to display the page.\n\n\n\n\n\nDiagram of the Web\n\n\n\n\n\nOn a side note, it’s important to distinguish the Internet from the Web. The Web, originally called the World Wide Web, is just one option to share information over the Internet. What characterizes the Web is that it allows documents to be linked to one another using hypertext links or hyperlinks, thus forming a web of interconnected resources.\n\nIn Summary\n\nThe Web is a massive distributed information system connecting software and computers to share information.\nThe software and computers that form the Web are divided into two types: clients and servers.\nThe way clients and servers dialogue between each other is by following formal protocols of communication.\nThe main type of protocol that clients and servers use is the HyperText Transfer Protocol (HTTP).\nBut there are other ways in which computers can exchange information such as email, file transfer (FTP), and many others."
  },
  {
    "objectID": "1-03-http.html",
    "href": "1-03-http.html",
    "title": "3  Basics of HTTP",
    "section": "",
    "text": "In the preceding chapter you were given a high-level description about how the Web works. In this chapter we take the next step to give you a basic introduction to HTTP which is the protocol that servers and browsers use to communicate and exchange information on the Web."
  },
  {
    "objectID": "1-03-http.html#what-is-http",
    "href": "1-03-http.html#what-is-http",
    "title": "3  Basics of HTTP",
    "section": "3.1 What is HTTP?",
    "text": "3.1 What is HTTP?\nHTTP is the acronym for Hypertext Transfer Protocol. Perhaps the two most important terms in this name are Protocol and Transfer.\nAccording to the dictionary, a protocol is:\n\n“A system of rules that explain the correct conduct and procedures to be followed in formal situations”\n\nIn turn, a transfer protocol is a communications protocol:\n\n“A communications protocol is a system of digital rules for data exchange within or between computers.”\n\nSo, what is HTTP? Simply put, HTTP is the set of rules for transferring things such as text, images, sound, video and other multimedia files over the Web."
  },
  {
    "objectID": "1-03-http.html#a-quick-introduction-to-http",
    "href": "1-03-http.html#a-quick-introduction-to-http",
    "title": "3  Basics of HTTP",
    "section": "3.2 A quick introduction to HTTP",
    "text": "3.2 A quick introduction to HTTP\n\nWhenever you surf the web, your browser sends HTTP request messages\nthe HTTP requests are sent to Web servers\nweb servers handle these requests by returning HTTP response messages\nthe messages contain the requested resource(s)\n\n\n3.2.1 HTTP Example\nSuppose we open the browser in order to visit R project’s homepage\nhttps://www.r-project.org\nAlthough we don’t see it, there’s is a client-server dialogue taking place, illustrated in the diagram below:\n\n\n\n\n\nClient makes a request, and the serve responds\n\n\n\n\n\nUsing Chrome’s DevTools (developer tools), we can see the associated information related to the HTTP “conversation” between the client and the server. We provide the content of this dialogue in the following block:\n\n\n\n\n\nInspecting HTTP messages via DevTools\n\n\n\n\n\nThink of an HTTP request as a set of information sent to the server. When the server receives the request, it (the server) processes the information and provides a response back to the client.\nWhen you visit a URL in your web browser, say R’s project website (https://www.r-project.org), an HTTP request is made and the response is rendered by the browser as the website you see. Although we don’t see the “dialogue” between client and server, it is possible to inspect this interaction using the development tools in a browser such as Chrome’s DevTools (like the screenshot above).\n\n\n\n\n\nDevTools menu tab for inspecting HTTP messages\n\n\n\n\nThe above is a screen-capture in which we can see that the request is composed of a URL (R’s project website), and a request method (GET) which is what the browser employs to access a website.\n\n\n3.2.2 HTTP Request\nThere are several components of an HTTP request (see figure below), but we will focus on the most relevant:\n\nURL: the address or endpoint for the request\nHTTP method or verb: a specific method invoked on the endpoint (GET, POST, DELETE, PUT)\nHeaders: additional data sent to the server, such as who is making the request and what type of response is expected\nBody: data sent to the server outside of the headers, common for POST and PUT requests\n\n\n\n\n\n\nHTTP request headers\n\n\n\n\n\n\n3.2.3 HTTP Response\nThe response headers include the HTTP status code that informs the client how the request was received. There are also other details about the content delivered by the server. In the above example accessing www.r-project.com, we can see the status code success 200, along with other details about the response content. Notice that the returned content is HTML. This HTML content is what the browser renders into a webpage.\n\n\n\n\n\nHTTP response headers"
  },
  {
    "objectID": "1-03-http.html#anatomy-of-an-http-message",
    "href": "1-03-http.html#anatomy-of-an-http-message",
    "title": "3  Basics of HTTP",
    "section": "3.3 Anatomy of an HTTP message",
    "text": "3.3 Anatomy of an HTTP message\nHTTP messages consist of 2 parts (separated by a blank line)\n\nA message header\n\nthe first line in the header is the request/response line\nthe rest of the lines are headers formed of name:value pairs\n\nAn optional message body\n\nThe client (your browser) sends a request to the server:\nGET / HTTP/1.1\nUser-Agent: curl/7.24.0 (x86_64-apple-darwin12.0) libcurl/7.24.0 OpenSSL/0.9.8y zlib/1.2.5\nHost: r-project.org\nAccept: */*\n\nThe first line is the request line which contains: GET / HTTP/1.1\nThe rest of the headers are just name:value pairs, e.g. Host: r-project.org\n\nThe server sends a response to the client:\nHTTP/1.1 301 Moved Permanently\nDate: Thu, 01 May 2014 16:54:43 GMT\nServer: Apache/2.2.22 (Debian)\nLocation: http://www.r-project.org/\nVary: Accept-Encoding\nContent-Length: 312\nContent-Type: text/html; charset=iso-8859-1\n \n<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n<html>\n...\n</html>\n\nThe first line is the status line which contains: GET / HTTP/1.1\nThe next lines contain header values\nThe body message appears after the blank line, in this case is the content of the HTML page\n\n\n3.3.1 HTTP Methods\nHere’s a table with HTTP methods, and their descriptions\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nGET\nretrieves whatever information is identified by the Request-URI\n\n\nPOST\nrequest with data enclosed in the request body\n\n\nHEAD\nidentical to GET except that the server MUST NOT return a message-body in the response\n\n\nPUT\nrequests that the enclosed entity be stored under the supplied Request-URI\n\n\nDELETE\nrequests that the origin server delete the resource identified by the Request-URI\n\n\nTRACE\ninvokes a remote, application-layer loop-back of the request message\n\n\nCONNECT\nfor use with a proxy that can dynamically switch to being a tunnel\n\n\n\n\nSo far we’ve seen that:\n\nThe HTTP protocol is a standardized method for transferring data or documents over the Web\nThe clients’ requests and the servers’ responses are handled via the HTTP protocol\nThere are 2 types of HTTP messages: requests and responses\nWe don’t actually see HTTP messages but they are there behind the scenes"
  },
  {
    "objectID": "2-01-xml.html",
    "href": "2-01-xml.html",
    "title": "4  Basics of XML",
    "section": "",
    "text": "The goal of this chapter is to give you a crash introduction to XML so that you can get a good grasp of this format for the rest of the book."
  },
  {
    "objectID": "2-01-xml.html#what-is-xml",
    "href": "2-01-xml.html#what-is-xml",
    "title": "4  Basics of XML",
    "section": "4.1 What is XML?",
    "text": "4.1 What is XML?\nXML stands for eXtensible Markup Language\nLet’s dissect the meaning of this acronym. On one hand, XML is a markup language. which means, XML defines a set of rules for encoding information in a format that is both human-readable and machine-readable.\nCompared to other types of markup languages (e.g LaTeX, Markdown), XML is used to describe data. To be more precise, XML is a standard for the semantic, hierarchical representation of data. This is an important aspect of XML and any of its dialects, because data is represented following a hierarchy.\nFor instance, one way to organize data is in a table. Conceptually, all elements are stored in cells of a grid structure of rows and columns. Another way to organize data is with hierarchies, that can be visually represented with tree like structures. This latter form of organizing data is what XML uses.\nThe second aspect, “extensible”, means that we can define any number of new formats to represent any kind of data. Therefore, it is extensible. This is a very interesting aspect of XML because it provides a flexible framework to create new formats for describing and representing data.\n\nComments\nBefore moving on, we want to clarify some key terms.\nA markup is a sequence of characters or other symbols inserted at certain places in a document to indicate either:\n\nhow the content should be displayed when printed or in screen\ndescribe the document’s structure\n\nA Markup Language is a system for annotating (i.e. marking) a document in a way that the content is distinguished from its representation (e.g. LaTeX, PostScript, HTML, SVG)\n\n\n4.1.1 Marks in XML\nIn XML (as well as in HTML) the marks (also known as tags) are defined using angle brackets: < >.\nFor example:\n<mark>Text marked with special tag</mark>\nThe concept of extensibility means that we can define our own marks, the order in which they occur, and how they should be processed. For example we could define marks such as:\n\n<my_mark>\n<awesome>\n<boring>\n<pathetic>\n\nBefore moving on, we should mention that XML is NOT:\n\na programming language\na network transfer protocol\na database\n\nInstead, XML is:\n\nmore than a markup language\na generic language that provides structure and syntax for representing any type of information\na meta-language: it allows us to create or define other languages\n\nHere are some famous examples of XML dialects:\n\nKML (Keyhole Markup Language) for describing geo-spatial information used in Google Earth, Google Maps, Google Sky\nSVG (Scalable Vector Graphics) for visual graphical displays of two-dimensional graphics with support for interactivity and animation\nPMML (Predictive Model Markup Language) for describing and exchanging models produced by data mining and machine learning algorithms\nRSS (Rich Site Summary) feeds for publishing blog entries\nSDMX (Statistical Data and Metadata Exchange) for organizing and exchanging statistical information\nSBML (Systems Biology Markup Language) for describing biological systems\n\n\n\n4.1.2 Minimalist Example\nLet’s consider a handful of XML examples using one of my favorite movies: Good Will Hunting, a 1997 American psychological drama film directed by Gus Van Sant, and written by Ben Affleck and Matt Damon.\n\n\n\n\n\nGood Will Hunting (Directed by Gus Van Sant, 1997)\n\n\n\n\n\nUltra Simple example\nLet’s see an ultra simple XML example:\n<movie>\n  Good Will Hunting\n</movie>\n\none single element movie\nstart-tag: <movie>\nend-tag: </movie>\ncontent: Good Will Hunting\n\n\n\nElements with attributes\nXML elements can have attributes, for example:\n<movie mins=\"126\" lang=\"en\">\n  Good Will Hunting\n</movie>\n\nattributes: mins (minutes) and lang (language)\nattributes are attached to the element’s start tag\nattribute values must be quoted!\n\n\n\nElements within other elements\nXML elements may contain other elements, for example:\n<movie mins=\"126\" lang=\"en\">\n  <title>Good Will Hunting</title>\n  <director>Gus Van Sant</director>\n  <year>1998</year>\n  <genre>drama</genre>\n</movie>\n\nan xml element may contain other elements\nmovie contains several elements: title, director, year, genre\n\n\n\nMore Embedded elements\nAs you can tell, the xml element movie has a now a hierarchy. We can make it more interesting by including more elements inside director.\n<movie mins=\"126\" lang=\"en\">\n  <title>Good Will Hunting</title>\n  <director>\n    <first_name>Gus</first_name>\n    <last_name>Van Sant</last_name>\n  </director>\n  <year>1998</year>\n  <genre>drama</genre>\n</movie>\nFormally, we say that director has two child elements: first_name and last_name.\n\n\nTree Structure in XML\nWe can graphically display the structure of an XML document with a tree diagram, like the following one:\n\n\n\n\n\nXML tree structure\n\n\n\n\n\nAn XML document can be represented with a tree structure\nAn XML document must have one single Root element\nThe Root may contain child elements\nA child element may contain subchild elements\n\n\n\n\n4.1.3 Well Formedness\nWe say that an XML document is well-formed when it obeys the basic syntax rules of XML. Some of those rules are:\n\none root element containing the rest of elements\nproperly nested elements\nself-closing tags\nattributes appear in start-tags of elements\nattribute values must be quoted\nelement names and attribute names are case sensitive\n\nDoes it matter if an XML document is not Well-formed? Not well-formed XML documents produce potentially fatal errors or warnings when parsed.\nKeep in mind that documents may be well-formed but not valid. Well-formed just guarantees that the document meets the basic XML structure, not that the content is valid.\n\n\n4.1.4 Additional XML Elements\nSome Additional Elements\n<?xml version=\"1.0\"? encoding=\"UTF-8\" ?>\n<![CDATA[ a > 5 & b < 10 ]]>\n<?GS print(format = TRUE)>\n<!DOCTYPE Movie>\n<!-- This is a commet -->\n<movie mins=\"126\" lang=\"en\">\n  <title>Good Will Hunting</title>\n  <director>\n    <first_name>Gus</first_name>\n    <last_name>Van Sant</last_name>\n  </director>\n  <year>1998</year>\n  <genre>drama</genre>\n</movie>\nThe following table lists some of the common additional XML elements:\n\n\n\n\n\n\n\n\nMarkup\nName\nDescription\n\n\n\n\n<?xml >\nXML Declaration\nIdentifies content as an XML document\n\n\n<?PI >\nProcessing Instruction\nProcessing instructions passed to application PI\n\n\n<!DOCTYPE >\nDocument-type Declaration\nDefines the structure of an XML document\n\n\n<![CDATA[ ]]>\nCDATA Character Data\nAnything inside a CDATA is ignored by the parser\n\n\n<!--  -->\nComment\nFor writing comments\n\n\n\n\n\n4.1.5 Another Example\nLet’s go back to the movie example, but now let’s see how the content of our hypothetical XML document should look like:\n<?xml version=\"1.0\"?>\n<!DOCTYPE movies>\n<movie mins=\"126\" lang=\"en\">\n  <!-- this is a comment -->\n  <title>Good Will Hunting</title>\n  <director>\n    <first_name>Gus</first_name>\n    <last_name>Van Sant</last_name>\n  </director>\n  <year>1998</year>\n  <genre>drama</genre>\n</movie>\nEach Node can have\n\na Name\nany number of attributes\noptional content\nother nested elements\n\n\n\n4.1.6 Wrap-Up\nAbout XML\n\ndesigned to store and transfer data\ndesigned to be self-descriptive\ntags are not predefined and can be extended\na generic language that provides structure and syntax for many markup dialects\nis a syntax or format for defining markup languages\na standard for the semantic, hierarchical representation of data\nprovides a general approach for representing all types of information dialects"
  },
  {
    "objectID": "2-02-parsing-xml.html",
    "href": "2-02-parsing-xml.html",
    "title": "5  Parsing XML",
    "section": "",
    "text": "The goal of this chapter is to describe how we can parse XML content with the R package xml2\nYou will need the following packages\nWe’ll cover a variety of situations you most likely will find yourself dealing with:"
  },
  {
    "objectID": "2-02-parsing-xml.html#what-is-parsing",
    "href": "2-02-parsing-xml.html#what-is-parsing",
    "title": "5  Parsing XML",
    "section": "5.1 What is parsing?",
    "text": "5.1 What is parsing?\nGetting data from the web often involves reading and processing content from XML and HTML documents. This is known as parsing.\nThe dictionary defines “parse” as:\n\nanalyze (a sentence) into its parts and describe their syntactic roles.\n\nIn regards to “computing”, parse has to do with:\n\nanalyze (a string or text) into logical syntactic components, typically in order to test conformability to a logical grammar.\nan act of or the result obtained by parsing a string or a text.\n\nAccording to Wikipedia, a parser is:\n\nA parser is a software component that takes input data (frequently text) and builds a data structure —often some kind of parse tree, abstract syntax tree or other hierarchical structure— giving a structural representation of the input, checking for correct syntax in the process"
  },
  {
    "objectID": "2-02-parsing-xml.html#r-package-xml2",
    "href": "2-02-parsing-xml.html#r-package-xml2",
    "title": "5  Parsing XML",
    "section": "5.2 R package \"xml2\"",
    "text": "5.2 R package \"xml2\"\nThe package \"xml2\" is designed for one major purpose, namely, to parse XML and HTML content. Remember that HTML is one the countless XML dialects.\nAs of this writing, \"xml2\" has minimal functionality for writing content in XML. Hadley Wickham has mentioned that he plans to add more functions for writing XML. So it is possible that in the future, \"xml2\" integrates more writing-XML functionality. Having said that, we will focus exclusively on reading XML content.\nWe’ll cover 4 major types of tasks that we can perform with \"xml2\"\n\nparsing (ie reading) xml / html content\nobtaining descriptive information about parsed contents\nnavigating the tree structure (i.e. accessing its components)\nquerying and extracting data from parsed contents\n\n\n5.2.1 Parsing Functions\nThere are two main parsing functions:\n\nread_xml()\nread_html()\n\nFor XML files in general, you should use read_xml(). For HTML files, then it’s better to use read_html() because it is more robust, and can handle no well-formed HTML files, which are not uncommon to deal with in practice.\nThe main input for these reading functions is either a string, an R connection, or a raw vector.\nThe string can be either a path, a URL or literal xml. URL’s will be converted into connections either using base::url() or, if installed, curl::curl(). Local paths ending in .gz, .bz2, .xz, .zip will be automatically uncompressed.\nBoth read_xml() and read_html() return an object of class \"xml_document\".\nLet’s see an example. Consider one of the examples from the previous chapter, for instance some content in XML:\n<movie mins=\"126\" lang=\"en\">\n  <title>Good Will Hunting</title>\n  <director>\n    <first_name>Gus</first_name>\n    <last_name>Van Sant</last_name>\n  </director>\n  <year>1998</year>\n  <genre>drama</genre>\n</movie>\nFor illustration purposes, let’s take the XML content, treating it as a single character string, that we then pass to read_xml():\n\n# toy example with xml string\nmovie <- read_xml(\n\"<movie>\n<title>Good Will Hunting</title>\n<director>\n<first_name>Gus</first_name>\n<last_name>Van Sant</last_name>\n</director>\n<year>1998</year>\n<genre>drama</genre>\n</movie>\")\n\nmovie\n\n{xml_document}\n<movie>\n[1] <title>Good Will Hunting</title>\n[2] <director>\\n  <first_name>Gus</first_name>\\n  <last_name>Van Sant</last_n ...\n[3] <year>1998</year>\n[4] <genre>drama</genre>\n\n\nAs we mention, the movie is an XML object:\n\nclass(movie)\n\n[1] \"xml_document\" \"xml_node\"    \n\n\nThis type of object has an internal structure in order to maintain the hierarchical tree-structure of any XML content."
  },
  {
    "objectID": "2-02-parsing-xml.html#working-with-parsed-documents",
    "href": "2-02-parsing-xml.html#working-with-parsed-documents",
    "title": "5  Parsing XML",
    "section": "5.3 Working with parsed documents",
    "text": "5.3 Working with parsed documents\nHaving parsed an XML / HTML document, we can use 2 main functions to start working on the tree structure:\n\nxml_root() gets access to the root node and its elements\nxml_children() gets access to the children nodes of a given node\n\n\n5.3.1 Example with a basic XML document\nHere’s some content: a movie elements in XML syntax\n\n\n\n\n\nXML Movie\n\n\n\n\n\nThe following figure identifies the main nodes:\n\n\n\n\n\nXML Movie nodes\n\n\n\n\n\nBelow is an abstract representation of an XML file, and its main nodes\n\n\n\n\n\nFunctions of xml2\n\n\n\n\n\n\n5.3.2 More Functions in \"xml2\"\nIn addition to xml_root() and xml_children(), there are other functions to parse the various kinds of content within a given node.\nHere’s a table with the main navigation functions. Keep in mind that the applicability of the functions depends on the class of objects we are working on.\n\n\n\nFunction\nDescription\n\n\n\n\nxml_root()\nReturns root node\n\n\nxml_children()\nReturns children nodes\n\n\nxml_child()\nReturns specified children number\n\n\nxml_name()\nReturns name of a node\n\n\nxml_contents()\nReturns contents of a node\n\n\nxml_text()\nReturns text\n\n\nxml_length()\nReturns number of children nodes\n\n\nxml_parents()\nReturns set of parent nodes\n\n\nxml_siblings()\nReturns set of sibling nodes\n\n\n\n\n\n\n5.3.3 Navigation of XML / HTML Tree\nLet’s consider the following XML content:\n<movies>\n     <movie mins=\"126\" lang=\"eng\">\n        <title>Good Will Hunting</title>\n        <director>\n           <first_name>Gus</first_name>\n           <last_name>Van Sant</last_name>\n        </director>\n        <year>1998</year>\n        <genre>drama</genre>\n     </movie>\n     <movie mins=\"106\" lang=\"spa\">\n        <title>Y tu mama tambien</title>\n        <director>\n           <first_name>Alfonso</first_name>\n           <last_name>Cuaron</last_name>\n        </director>\n        <year>2001</year>\n        <genre>drama</genre>\n     </movie>\n  </movies>\nTheis content can be depicted in the following tree-diagram:\n\n\n\n\n\nXML movies tree\n\n\n\n\n\nLet’s create a character vector to store the XML content:\n\n# toy example with xml string\nxml_string <- c(\n  '<?xml version=\"1.0\" encoding=\"UTF-8\"?>',\n  '<movies>',\n  '<movie mins=\"126\" lang=\"eng\">',\n  '<title>Good Will Hunting</title>',\n  '<director>',\n  '<first_name>Gus</first_name>',\n  '<last_name>Van Sant</last_name>',\n  '</director>',\n  '<year>1998</year>',\n  '<genre>drama</genre>',\n  '</movie>',\n  '<movie mins=\"106\" lang=\"spa\">',\n  '<title>Y tu mama tambien</title>',\n  '<director>',\n  '<first_name>Alfonso</first_name>',\n  '<last_name>Cuaron</last_name>',\n  '</director>',\n  '<year>2001</year>',\n  '<genre>drama</genre>',\n  '</movie>',\n  '</movies>')\n\nLet’s parse the content. To do this, we must first create a single contiguous xml string, which is done with paste() and its collapse = '' argument:\n\n# parsing xml string\ndoc <- read_xml(paste(xml_string, collapse = ''))\n\ndoc\n\n{xml_document}\n<movies>\n[1] <movie mins=\"126\" lang=\"eng\">\\n  <title>Good Will Hunting</title>\\n  <dir ...\n[2] <movie mins=\"106\" lang=\"spa\">\\n  <title>Y tu mama tambien</title>\\n  <dir ...\n\n\nAnd let’s navigate the tree structure. We begin with xml_root() to get access to the root node:\n\n# root node\nmovies <- xml_root(doc)\nmovies\n\n{xml_document}\n<movies>\n[1] <movie mins=\"126\" lang=\"eng\">\\n  <title>Good Will Hunting</title>\\n  <dir ...\n[2] <movie mins=\"106\" lang=\"spa\">\\n  <title>Y tu mama tambien</title>\\n  <dir ...\n\n\nIt turns out that doc and movies are actually identical:\n\nidentical(doc, movies)\n\n[1] TRUE\n\n\nWe use the xml_length() to know how many elements or nodes are in the root node:\n\n# parsing xml string\nxml_length(doc)\n\n[1] 2\n\n\nwhich confirms what we know about the movies string that contains two movie elements: one node for “Good Will Hunting” and another node for “Y tu mama tambien”.\nThe function xml_children() allows you to access the children nodes:\n\nxml_children(doc)\n\n{xml_nodeset (2)}\n[1] <movie mins=\"126\" lang=\"eng\">\\n  <title>Good Will Hunting</title>\\n  <dir ...\n[2] <movie mins=\"106\" lang=\"spa\">\\n  <title>Y tu mama tambien</title>\\n  <dir ...\n\n\nNotice that the output is an object of class \"xml_nodeset\". To access a specific node, you use the function xml_child(). In this example, the node for movie “Good Will Hunting” corresponds to the first node, and we pass this value to the search argument:\n\nxml_child(doc, search = 1)\n\n{xml_node}\n<movie mins=\"126\" lang=\"eng\">\n[1] <title>Good Will Hunting</title>\n[2] <director>\\n  <first_name>Gus</first_name>\\n  <last_name>Van Sant</last_n ...\n[3] <year>1998</year>\n[4] <genre>drama</genre>\n\n\nLikewise, the second node (“Y tu mama tambien”) is accessed by specifying the argument search = 2:\n\nxml_child(doc, search = 2)\n\n{xml_node}\n<movie mins=\"106\" lang=\"spa\">\n[1] <title>Y tu mama tambien</title>\n[2] <director>\\n  <first_name>Alfonso</first_name>\\n  <last_name>Cuaron</last ...\n[3] <year>2001</year>\n[4] <genre>drama</genre>\n\n\nThis is the view of the tree structure so far:\n\n\n\n\n\nTwo movie nodes\n\n\n\n\n\nInspecting first node\nLet’s go inside the first node, and store the content in the object good_will\n\n# first child\ngood_will <- xml_child(doc, search = 1)\ngood_will\n\n{xml_node}\n<movie mins=\"126\" lang=\"eng\">\n[1] <title>Good Will Hunting</title>\n[2] <director>\\n  <first_name>Gus</first_name>\\n  <last_name>Van Sant</last_n ...\n[3] <year>1998</year>\n[4] <genre>drama</genre>\n\n\nand let’s do the same for the second node, storing the content in the object tu_mama:\n\n# second child\ntu_mama <- xml_child(doc, search = 2)\ntu_mama\n\n{xml_node}\n<movie mins=\"106\" lang=\"spa\">\n[1] <title>Y tu mama tambien</title>\n[2] <director>\\n  <first_name>Alfonso</first_name>\\n  <last_name>Cuaron</last ...\n[3] <year>2001</year>\n[4] <genre>drama</genre>\n\n\nWe can then again apply xml_children() on each node to see what children nodes good_will and tu_mama have:\n\n# children of good_will\nxml_children(good_will)\n\n{xml_nodeset (4)}\n[1] <title>Good Will Hunting</title>\n[2] <director>\\n  <first_name>Gus</first_name>\\n  <last_name>Van Sant</last_n ...\n[3] <year>1998</year>\n[4] <genre>drama</genre>\n\n\n\n# children of tu_mama\nxml_children(tu_mama)\n\n{xml_nodeset (4)}\n[1] <title>Y tu mama tambien</title>\n[2] <director>\\n  <first_name>Alfonso</first_name>\\n  <last_name>Cuaron</last ...\n[3] <year>2001</year>\n[4] <genre>drama</genre>\n\n\nThe visual diagram for good_will depicts the four nodes:\n\n\n\n\n\nChildren nodes of ‘Good Will Hunting’\n\n\n\n\n\nThe code below shows a deeper inspection of good_will. The function xml_name() gives the name of a node.\n\n# name of an element\nxml_name(good_will)\n\n[1] \"movie\"\n\n\nThe function xml_attrs() gives you the attributes of a node. In this case, the node of good_will has to attributes \"mins\" and \"lang\"\n\n# attributes\nxml_attrs(good_will)\n\n mins  lang \n\"126\" \"eng\" \n\n\nAs we previously saw, xml_length() gives the number of children nodes inside a given node:\n\n# how many children\nxml_length(good_will)\n\n[1] 4\n\n\nLikewise, we can move along the children nodes, and find information about their names, their subchildren, and so on:\n\n# name of children (of good_will)\nxml_name(xml_children(good_will))\n\n[1] \"title\"    \"director\" \"year\"     \"genre\"   \n\n# good_will title\nxml_child(good_will, \"title\")\n\n{xml_node}\n<title>\n\n\n\n# good_will title\ntitle1 <- xml_child(good_will, \"title\")\ntitle1\n\n{xml_node}\n<title>\n\n# content good_will title\nxml_contents(title1)\n\n{xml_nodeset (1)}\n[1] Good Will Hunting\n\n# text good_will title\nxml_text(title1)\n\n[1] \"Good Will Hunting\"\n\n\n\n\n5.3.3.1 Inspecting director node\n\n# good_will director\ndir1 <- xml_child(good_will, \"director\")\ndir1\n\n{xml_node}\n<director>\n[1] <first_name>Gus</first_name>\n[2] <last_name>Van Sant</last_name>\n\n\n\nxml_children(dir1)\n\n{xml_nodeset (2)}\n[1] <first_name>Gus</first_name>\n[2] <last_name>Van Sant</last_name>\n\n\nTo extract just the text, we use xml_text():\n\nxml_text(dir1)\n\n[1] \"GusVan Sant\"\n\n\nThe visual diagram for good_will with its director node is depicted in this figure:\n\n\n\n\n\nDirector node of ‘Good Will Hunting’"
  },
  {
    "objectID": "2-03-xpath.html",
    "href": "2-03-xpath.html",
    "title": "6  XPath Language",
    "section": "",
    "text": "In the preceding chapter you learned about the main functions in \"xml\" that allows us to parse XML and HTML documents. While those functions can be quite useful to navigate through the elements of an XML document, their default usage can be a bit limiting.\nThe real parsing power comes from the ability to locate nodes and extract information from them. For this, we need to be able to perform queries on the parsed content. The solution is provided by XPath, which is a language to navigate through elements and attributes in an XML/HTML document."
  },
  {
    "objectID": "2-03-xpath.html#what-is-xpath",
    "href": "2-03-xpath.html#what-is-xpath",
    "title": "6  XPath Language",
    "section": "6.1 What is XPath?",
    "text": "6.1 What is XPath?\nXPath is a language for finding information in an XML document. It works by identifying patterns to match data or content. To be more precise, XPath uses path expressions to select nodes in an XML document by taking into account the tree structure of XML based on:\n\nnode names\nnode content\na node’s relationship to other nodes\n\n\n6.1.1 XPath Syntax\nThe key concept is knowing how to write XPath expressions. XPath expressions have a syntax similar to the way files are located in a hierarchy of directories and folders in a computer file system. For instance:\n/movies/movie\nis the XPath expression to locate the movie children in the movies (root) element\n\n\n6.1.2 Selecting Nodes\nThe main symbols to define path expressions are:\n\n\n\nSymbol\nDescription\n\n\n\n\n/\nselects from the root node\n\n\n//\nselects nodes anywhere\n\n\n.\nselects the current node\n\n\n..\nselects the parent of the current node\n\n\n@\nselects attributes\n\n\n[]\nsquare brackets to indicate attributes\n\n\n*\nmatches any element node\n\n\n@*\nmatches any attribute node\n\n\n\nFor instance:\n\n\n\n\n\n\n\nExample\nDescription\n\n\n\n\n/node\nselects top level node\n\n\n//node\nselects nodes at any level\n\n\nnode[@attr]\nnode that has an attribute named attr\n\n\nnode[@attr=\"abc\"]\nnode that has an attribute named attr with value \"abc\"\n\n\nnode/@attr\nvalue of an attribute attr in node with such attribute\n\n\nnode/*\nany (child) element in node\n\n\nnode/@*\nvalue of any attribute in node"
  },
  {
    "objectID": "2-03-xpath.html#xpath-examples",
    "href": "2-03-xpath.html#xpath-examples",
    "title": "6  XPath Language",
    "section": "6.2 XPath Examples",
    "text": "6.2 XPath Examples\nTo make things less abstract, let’s bring back the movies XML document containing two movies Good Will Hunting and Y tu mama tambien\n\n\n\n\n\nXML movies\n\n\n\n\n\nThe following diagrams illustrate different XPath expressions to match and select nodes based on either: their names, their content, or their relationship to other nodes.\n\n\n\n\n\n“movie” nodes\n\n\n\n\n\n\n\n\n\n“title” nodes\n\n\n\n\n\n\n\n\n\nAny nodes of “movies” node\n\n\n\n\n\n\n\n\n\nAnother way to select “title” nodes\n\n\n\n\n\n\n\n\n\n“first name” nodes\n\n\n\n\n\n\n\n\n\n“movie/director” anywhere in the XML tree\n\n\n\n\n\n\n\n\n\n“last name” nodes anywhere in the XML tree\n\n\n\n\n\n\n\n\n\n“movie” nodes having “lang” attribute\n\n\n\n\n\n\n\n\n\n“movie” nodes with “lang” attribute having value “spa”\n\n\n\n\n\n\n\n\n\n“title” node of movie with spanish language attribute\n\n\n\n\n\n\n\n\n\nAny atrribute of “movie” nodes"
  },
  {
    "objectID": "2-03-xpath.html#using-xpath-functions",
    "href": "2-03-xpath.html#using-xpath-functions",
    "title": "6  XPath Language",
    "section": "6.3 Using XPath Functions",
    "text": "6.3 Using XPath Functions\nThe R package \"xml2\" provides a large number of functions that admit XPath expressions; these functions have the xpath argument. The following code snippets, based on the above pattern examples, show you how to use some of these functions.\n\n# toy example with xml string\nxml_string <- c(\n  '<?xml version=\"1.0\" encoding=\"UTF-8\"?>',\n  '<movies>',\n  '<movie mins=\"126\" lang=\"eng\">',\n  '<title>Good Will Hunting</title>',\n  '<director>',\n  '<first_name>Gus</first_name>',\n  '<last_name>Van Sant</last_name>',\n  '</director>',\n  '<year>1998</year>',\n  '<genre>drama</genre>',\n  '</movie>',\n  '<movie mins=\"106\" lang=\"spa\">',\n  '<title>Y tu mama tambien</title>',\n  '<director>',\n  '<first_name>Alfonso</first_name>',\n  '<last_name>Cuaron</last_name>',\n  '</director>',\n  '<year>2001</year>',\n  '<genre>drama</genre>',\n  '</movie>',\n  '</movies>')\n\n# parsing xml string\ndoc = read_xml(paste(xml_string, collapse = ''))\n\n\n# movie children (from root node)\nmovie_nodes = xml_find_all(doc, xpath = \"/movies/movie\")\nmovie_nodes\n\n{xml_nodeset (2)}\n[1] <movie mins=\"126\" lang=\"eng\">\\n  <title>Good Will Hunting</title>\\n  <dir ...\n[2] <movie mins=\"106\" lang=\"spa\">\\n  <title>Y tu mama tambien</title>\\n  <dir ...\n\n\n\n# title children (from root node)\ntitle_nodes = xml_find_all(doc, xpath = \"/movies/movie/title\")\ntitle_nodes\n\n{xml_nodeset (2)}\n[1] <title>Good Will Hunting</title>\n[2] <title>Y tu mama tambien</title>\n\n\n\n# text content of title_nodes\nxml_text(title_nodes)\n\n[1] \"Good Will Hunting\" \"Y tu mama tambien\"\n\n\n\n# director children (from any movie element)\ndirector_nodes = xml_find_all(doc, \"//movie/director\")\ndirector_nodes\n\n{xml_nodeset (2)}\n[1] <director>\\n  <first_name>Gus</first_name>\\n  <last_name>Van Sant</last_n ...\n[2] <director>\\n  <first_name>Alfonso</first_name>\\n  <last_name>Cuaron</last ...\n\n\n\n# text content of director_nodes\nxml_text(director_nodes)\n\n[1] \"GusVan Sant\"   \"AlfonsoCuaron\"\n\n\n\n# last_name (from anywhere in the tree)\nlast_name_nodes = xml_find_all(doc, \"//last_name\")\nlast_name_nodes\n\n{xml_nodeset (2)}\n[1] <last_name>Van Sant</last_name>\n[2] <last_name>Cuaron</last_name>\n\n\n\n# text of last_name (from anywhere in the tree)\nxml_text(last_name_nodes)\n\n[1] \"Van Sant\" \"Cuaron\"  \n\n\n\n# title node of movie with attribute lang='spa'\ntitle_spa = xml_find_all(doc, \"/movies/movie[@lang='spa']/title\")\ntitle_spa\n\n{xml_nodeset (1)}\n[1] <title>Y tu mama tambien</title>\n\n\n\n# text content of title_spa\nxml_text(title_spa)\n\n[1] \"Y tu mama tambien\""
  },
  {
    "objectID": "2-04-html.html",
    "href": "2-04-html.html",
    "title": "7  Basics of HTML",
    "section": "",
    "text": "The goal of this chapter is to give you a crash introduction to HTML, so you can get a good grasp of this format before moving to the next chapter."
  },
  {
    "objectID": "2-04-html.html#a-quick-introduction-to-html",
    "href": "2-04-html.html#a-quick-introduction-to-html",
    "title": "7  Basics of HTML",
    "section": "7.1 A quick introduction to HTML",
    "text": "7.1 A quick introduction to HTML\nHTML is not a programming language; it is simply a markup language, which means it is a syntax for identifying and describing the elements of a document such as headings, paragraphs, lists, tables, images, hyperlinks, etc. Technically, HTML is an XML dialect.\nSay we visit R’s official website (screencapture below).\n\n\n\n\n\nR project’s home page\n\n\n\n\n\nThe visually rich and interactive pages we see on the Web are based on plain text files referred to as source files. To look at the actual HTML content behind R’s homepage, you need to get access to the source code option in your browser. If you are using Chrome, go to the View tab in the menu bar, then choose the Developer option, and finally click on View Source.\n\n\n\n\n\nView source code of a webpage in Chrome\n\n\n\n\n\nIf we take a look at the source file behind R’s homepage, we’ll discover the actual HTML content, depicted in the image below.\n\n\n\n\n\nHTML source code behind R project’s home page\n\n\n\n\n\nAs you can tell, the webpage is cleverly rendered by your browser that knows exactly how to take care of the content in the source file. If you are not familiar with HTML, some (if not most) of the text will look like gibberish to you right now. But it all has a specific structure and meaning.\nWhat you see on the browser is the result of the resources served by the server where R’s website is stored. Technically speaking, the resources should include an index.htmlfile, plus other files (stylesheet files, and image files)\n\n\n\n\n\nOther resources being linked in the home page\n\n\n\n\n\nIn particular, the following resources (different types of files) can be identified:\n\nindex.html\nfavicon-33x32.png\nfavicon-16x16.png\nbootstrap.min.css\nR.css\nRlogo.png\n\n\n7.1.1 HTML document structure\nLet’s study the structure of a basic HTML document. Below is a diagram with a simplified content of R’s webpage.\n\n\n\n\n\nHTML document structure\n\n\n\n\n\nThe first line of text is the document type declaration, which identifies this document as an HTML5 document. Then we have the html element which is the root element of the document, and it contains all the other elements.\nWithin the html element, we find two elements: the head and the body. The head element contains descriptive information such as the title, style sheets, scripts, and other meta information. The mandatory element inside the head is the title.\nThe body element contains everything that is displayed in the browser.\n\n\n7.1.2 HTML Syntax\nYou don’t need to memorize all possible HTML elements (or tags), but it’s important that you learn about their syntax and structure. So let’s describe the anatomy of html elements.\nHere’s an example with a <p> element which is the paragraph element. An HTML tag has an opening tag consisting of the tag name surrounded by angle brackets, that is, the <p> characters.\nUsually, you put tags around some content text. At the end of the tag there is the closing tag, in this case </p>. You know it’s a closing tag because it comes after the content, and it has a slash / before the p name. All closing tags have a slash in them.\n\n\n\n\n\nAnatomy of html elements\n\n\n\n\n\nNot all tags come in the form of a pair of matching tags (an opening and a closing tag). There are some tags that don’t have a closing tag. Perhaps the most common tag of this type is the <img> tag used for images. One example is the <img> tag for the R logo file in the homepage of R project:\n<img src=\"/Rlogo.png\"/>\nAs you can tell, the <img> tag does not have a closing tag; you can say that itself closes with a slash and the right angle bracket />.\nSome elements have attributes which allows you to specify additional information about an element. Attributes are declared inside the opening tag using special keywords. We assign values to attributes with the equals sign, and we specify the values inside quotations.\n\n\n\n\n\nAttributes and values in html tags\n\n\n\n\n\nIn the example above, a paragraph tag contains an attribute lang for language with a value of es for español or spanish.\nNotice also that the previous <img> element has an attribute src to indicate the source filename of the picture, in this case, \"/Rlogo.png\".\n\n\n7.1.3 What the browser does\nThe browser (e.g. Chrome, Safari, Firefox) reads the HTML, interprets all the tags, and renders the content accordingly. Recall that tags tell browser about the structure and meaning of the text. The browser identifies what parts are headings (e.g. <h1>, <h2>), what parts are paragraphs (e.g. <p>), what parts are lists (e.g. <ol>, <ul>), what text needs to be emphasized, and so on.\nThe HTML syntax tells the browser about the structure of a document: where the headings are, where the paragraphs are, what text is part of a list, and so on. How do browsers know this? Well, they have built-in default rules for how to render HTML elements. In addition to the default settings, HTML elements can be formatted in endless ways using what is called Cascade Style Sheets or CSS for short, that determine font types, colors, sizes, and many other visual aspects of a page.\n\n\n7.1.4 Web Scraping\nMany websites are secured by an SSL/TSL certificate, which you can identify by looking at the URL containing https (Hyper Text Transfer Protocol Secure). SSL stands for Secure Sockets Layer. This is a technology that keeps an internet connection secure and safeguards sensitive data that is being sent between a client and a server (for example, when you use your browser to shop in amazon) or server to server (for example, an application with payroll information). The SSL technology is currently deprecated and has been replaced entirely by TLS which stands for Transport Layer Security. Simply put, TSL also ensures data privacy the same way that SSL does. Since SSL is actually no longer used, this is the correct term that people should start using.\nHTTPS is a secure extension of HTTP. When a website uses HTTPS it means that the website is secured by an SSL/TLS certificate. Consequently, websites that install and configure an SSL/TLS certificate can use the HTTPS protocol to establish a secure connection with the server. Quote: “The details of the certificate, including the issuing authority and the corporate name of the website owner, can be viewed by clicking on the lock symbol on the browser bar.”\nWikipedia uses HTTPS. For instance, if we visit the entry for men’s long jump world record progression, the url is\nhttps://en.wikipedia.org/wiki/Men%27s_long_jump_world_record_progression\nIf we try to use functions like readHTMLTable from \"XML\" package, it will fail\nwiki <- 'https://en.wikipedia.org/wiki/Men%27s_long_jump_world_record_progression'\n\n# this fails\ntbls <- readHTMLTable(wiki)\nOne option to read the html tables and extract them as R data frames, is to first download the html file to your computer, and then use readHTMLTable() to scrape the tables:\n# desired url\nwiki <- 'https://en.wikipedia.org/wiki/Men%27s_long_jump_world_record_progression'\n\n# destination file\njump_html <- 'men-long-jump-records.html'\n\n# download file to your working directory\ndownload.file(wiki, jump_html)\n\ntbls <- readHTMLTable(jump_html)\nWe recommend using this option when:\n\nthe data fits in your computer, in this way you also have the raw data\nyou need to experiment and get to know the content, in order to decide which elements you will extract, which functions to use, what kind of processing operations or transformations you need to apply, etc.\nalso, downloading an HTML document save you from making innecessary requests that could get in trouble, and potentially be blocked by a server because you are overloading them with multiple requests."
  },
  {
    "objectID": "3-01-json.html",
    "href": "3-01-json.html",
    "title": "8  JSON Data",
    "section": "",
    "text": "The goal of this chapter is to provide an introduction for handling JSON data in R.\nWe’ll cover the following topics:"
  },
  {
    "objectID": "3-01-json.html#json-basics",
    "href": "3-01-json.html#json-basics",
    "title": "8  JSON Data",
    "section": "8.1 JSON Basics",
    "text": "8.1 JSON Basics\nJSON stands for JavaScript Object Notation and it is a format for representing data. More formally, we can say that it is a text-based way to store and transmit structured data. By using a simple syntax, you can easily store anything from a single number to strings, JSON-arrays, and JSON-objects using nothing but a string of plain text. As you will see, you can also nest arrays and objects, allowing you to create complex data structures."
  },
  {
    "objectID": "3-01-json.html#what-is-json",
    "href": "3-01-json.html#what-is-json",
    "title": "8  JSON Data",
    "section": "8.2 What is JSON?",
    "text": "8.2 What is JSON?\nLet’s first talk about what JSON is and why it is important.\nJSON is a data representation format very similar to XML. It’s used widely across the internet for almost every single API that you will access as well as for config files and things such as games and text editors. Its popularity is based on a handful of attractive aspects:\n\nIt’s extremely lightweight and compact to send back and forth due to the small size file;\nIt’s easy for both computers and people to read-and-write, compared to something like XML, since it’s much cleaner and there’s not as many opening and closing tags;\nIt maps very easily onto the data structures used by most programming languages (numbers, strings, booleans, nulls, arrays and associative arrays);\nIt also integrates very nicely with javascript since JSON is just a superset of javascript which means anything you write in JSON is valid javascript, which is a language used all throughout the web for front-end or back-end of applications.\nAlso, every single major language has some form of library or packages with built-in functionality to parse JSON strings into objects or classes in that language which makes working with JSON data extremely easy inside of a programming language.\n\nWhy should we care about JSON? When working with data from the Web, we’ll inevitably find some JSON data because it is commonly used in web applications to send data from the server to the browser. As a matter of fact, in your data science career you will be using JSON quite often, whether it is consuming an API, creating an API, or creating config files for you or other people to use for your application."
  },
  {
    "objectID": "3-01-json.html#understanding-json-syntax",
    "href": "3-01-json.html#understanding-json-syntax",
    "title": "8  JSON Data",
    "section": "8.3 Understanding JSON Syntax",
    "text": "8.3 Understanding JSON Syntax\nLet’s now talk about the syntax used to store and organize data in JSON.\n\n8.3.1 Data Types\nThe first thing to talk about is the data types or values that JSON can represent. As we know, JSON is a data representation format, so we need to be able to represent certain data types within it. JSON supports the following types:\n\nstring (in double quotes)\nnumber (in any format whether they’re decimal numbers, integers, negative numbers, even numbers in scientific notation)\ntrue and false (booleans)\nnull\n\n\n\n8.3.2 Arrays\nJSON also supports arrays (in JSON Sense) which are sets of data types defined within brackets, and contains a comma-separated list of values. For example [1, 3, 3] or [\"computing\", \"with\", \"data\"], which can be a set of any of the data types listed above.\nWe typically use arrays when we have a set of unnamed values, this is why some people refer to them as ordered unnamed arrays. The closest R object to a JSON array would be a vector:\n\nJSON: [1, 2, 3, ... ]; -vs- R: c(1, 2, 3, ...)\nJSON: [true, true, false, ... ]; -vs- R: c(TRUE, TRUE, FALSE, ...)\n\n\n\n8.3.3 Objects\nAnother type of data container is the so-called JSON object, which is the most complex but also the most used type of object within JSON, and it allows you to represent values that are key-value pairs:\n{\"key\": \"value\"}\nYou use curly braces to define a JSON-object, and inside the braces you put key-value pairs. The key must be surrounded by double quotes, followed by a colon, followed by the value. The value can be a single data type, but it can also be a JSON-array (which in turn can contain a JSON-object). Because you have the association of a key with its value, these JSON structures are also referred to as associative arrays.\nFor example, say the key is \"year\" and the value 2000, then a simple JSON object will look like this:\n{\"year\": 2000}\nAnother example can be a key \"name\" and a value \"Jessica\":\n{\"name\": \"Jessica\"}\nIf you have multiple key-value pairs, you separate each of them with a comma:\n{\n  \"name1\": \"Nicole\",\n  \"name2\": \"Pleuni\",\n  \"name3\": \"Rori\"\n}\nA more complex object might look like the following example. In this case we have JSON-object that contains three key-value pairs. Each of the keys is a \"person\" and the associated pair corresponds to an array which in turn contains a JSON-object with two key-value pairs: the first name, and the last name:\n{\n  \"person1\": [\n    {\n      \"first\": \"Nicole\",\n      \"last\": \"Adelstein\"\n    }\n  ],\n  \"person2\": [\n    {\n      \"first\": \"Pleuni\",\n      \"last\": \"Pennings\"\n    }\n  ],\n  \"person3\": [\n    {\n      \"first\": \"Rori\",\n      \"last\": \"Rohlfs\"\n    }\n  ]\n}\nBecause the data inside a JSON object is formed of key-value pairs, you could think of them as named arrays.\nWhat do JSON-objects correspond to in R? Well, there’s not really a unique correspondence between a JSON-object and its equivalent in structure R. For instance, let’s bring back one of the JSON-objects previously discussed:\n{\n  \"name1\": \"Nicole\",\n  \"name2\": \"Pleuni\",\n  \"name3\": \"Rori\"\n}\nWe could use a named R vector to store the same data:\n# named vector in R\nc(\"name1\" = \"Nicole\", \"name2\" = \"Pleuni\", \"name3\" = \"Rori\")\nBut we could also use an R list:\n# named list in R\nlist(\"name1\" = \"Nicole\", \"name2\" = \"Pleuni\", \"name3\" = \"Rori\")\nKeep in mind that JSON-objects can be more complex than this basic example. Because JSON objects can contain any other type of JSON data structure in them, the similar container in R to a JSON-object is a list.\n\n\n8.3.4 Examples of JSON Data Containers\nHere’s a series of examples involving combinations of JSON arrays and objects.\nJSON containers can be nested. Here’s one example:\n{\n    \"name\": [\"X\", \"Y\", \"Z\"],\n    \"grams\": [300, 200, 500], \n    \"qty\": [4, 5, null],\n    \"new\": [true, false, true]\n}\nHere’s another example of nested containers:\n[\n    { \"name\": \"X\", \n      \"grams\": 300,\n      \"qty\": 4,\n      \"new\": true },\n    { \"name\": \"Y\",\n      \"grams\": 200,\n      \"qty\": 5,\n      \"new\": false },\n    { \"name\": \"Z\",\n      \"grams\": 500, \n      \"qty\": null,\n      \"new\": true}\n]\n\n\n8.3.5 Data Table Toy Example\nLet’s consider a less basic example with some tabular data set:\n\n\n\nName\nGender\nHomeland\nBorn\nJedi\n\n\n\n\nAnakin\nmale\nTatooine\n41.9BBY\nyes\n\n\nAmidala\nfemale\nNaboo\n46BBY\nno\n\n\nLuke\nmale\nTatooine\n19BBY\nyes\n\n\nLeia\nfemale\nAlderaan\n19BBY\nno\n\n\nObi-Wan\nmale\nStewjon\n57BBY\nyes\n\n\nHan\nmale\nCorellia\n29BBY\nno\n\n\nPalpatine\nmale\nNaboo\n82BBY\nno\n\n\nR2-D2\nunknown\nNaboo\n33BBY\nno\n\n\n\nHow can we store this tabular data in JSON format? There are several ways to represent this data in JSON format. One option could be a JSON-array containing JSON-objects. Each JSON-object represents an individual:\n    [\n        {\n         \"Name\": \"Anakin\",\n         \"Gender\": \"male\", \n         \"Homeworld\": \"Tatooine\",\n         \"Born\": \"41.9BBY\",\n         \"Jedi\": \"yes\"\n        },\n        {\n         \"Name\": \"Amidala\",\n         \"Gender\": \"female\", \n         \"Homeworld\": \"Naboo\",\n         \"Born\": 46BBY\",\n         \"Jedi\": \"no\"\n        },\n        ...\n        {\n         \"Name\": \"R2-D2\",\n         \"Gender\": \"unknown\",\n         \"Homeworld\": \"Naboo\",\n         \"Born\": \"33BBY\",\n         \"Jedi\": \"no\"\n        }\n    ]\nAnother way to represent the data in the table above is by using an object containing key-value pairs in which the keys are the names of the columns, and the pairs are arrays (the data values in each column).\n{\n  \"Name\": [ \"Anakin\", \"Amidala\", \"Luke\", ... , \"R2-D2\" ],\n  \"Gender\": [ \"male\", \"female\", \"male\", ... , \"unknown\" ],\n  \"Homeworld\": [ \"Tatooine\", \"Naboo\", \"Tatooine\", ... , \"Naboo\" ],\n  \"Born\": [ \"41.9BBY\", \"46BBY\", \"19BBY\", ... , \"33BBY\" ],\n  \"Jedi\": [ \"yes\", \"no\", \"yes\", ... , \"no\" ] \n}"
  },
  {
    "objectID": "3-02-jsonlite-package.html",
    "href": "3-02-jsonlite-package.html",
    "title": "9  JSON R packages",
    "section": "",
    "text": "R has 3 packages for working with JSON data\nAll packages provide 2 main functions, toJSON() and fromJSON(), that allow conversion to and from data in JSON format, respectively. We’ll focus on the functions from \"jsonlite\".\nFor illustration purposes, let us consider the package \"jsonlite\".\nThere are 2 primary functions in \"jsonlite\":"
  },
  {
    "objectID": "3-02-jsonlite-package.html#function-tojson",
    "href": "3-02-jsonlite-package.html#function-tojson",
    "title": "9  JSON R packages",
    "section": "9.1 Function toJSON()",
    "text": "9.1 Function toJSON()\nThe function jsonlite::toJSON() converts an R object to a string in JSON.\n\nExample: single number to JSON-array\nLet’s begin with a super simple example by passing a single data value to the function toJSON():\n\ntoJSON(pi, digits = 4)\n\n[3.1416] \n\n\n\n\nExample: vectors to JSON-arrays\nConsider the following vectors\n\nnum <- c(1, 2, 3, 4, 5)\nlts <- c('a', 'b', 'c', 'd', 'e')\n\nApplying toJSON() to the vectors num and lts produces JSON arrays:\n\ntoJSON(num)\n\n[1,2,3,4,5] \n\ntoJSON(lts)\n\n[\"a\",\"b\",\"c\",\"d\",\"e\"] \n\n\nThe argument pretty = TRUE allows you to obtain a JSON string with added indentation whitespace:\n\ntoJSON(num, pretty = TRUE)\n\n[1, 2, 3, 4, 5] \n\ntoJSON(lts, pretty = TRUE)\n\n[\"a\", \"b\", \"c\", \"d\", \"e\"] \n\n\nWhat about an R vector with named elements? For example, here’s a vector vec\n\nvec <- num\nnames(vec) <- lts\nvec\n\na b c d e \n1 2 3 4 5 \n\n\nConverting vec to JSON, we get:\n\ntoJSON(vec)\n\n[1,2,3,4,5] \n\n\nAs you can tell, the names of the elements in vec are lost in translation.\n\n\nExample: matrix to JSON-array\nHere’s another example from an matrix to a JSON array:\n\nmat <- matrix(9:1, nrow = 3, ncol = 3)\nmat\n\n     [,1] [,2] [,3]\n[1,]    9    6    3\n[2,]    8    5    2\n[3,]    7    4    1\n\n\ntoJSON() converts an R matrix into a JSON-array\n\ntoJSON(mat)\n\n[[9,6,3],[8,5,2],[7,4,1]] \n\n\nNotice that the returned output arranges the values of the matrix row-by-row, also referred to as row-major. This means that when the input is an R matrix, toJSON() uses its argument matrix = \"rowmajor\".\nYou can change the arrangement to column-major by specifying the argument matrix = \"columnmajor\":\n\ntoJSON(mat, matrix = \"columnmajor\")\n\n[[9,8,7],[6,5,4],[3,2,1]] \n\n\n\n\nExample: data frame to JSON-object\nWe can also use toJSON() on data frames. Here’s an example of an assembled data frame swdf which will be converted to a JSON-object:\n\n# toy data\nsw_data <- rbind(\n  c(\"Anakin\", \"male\", \"Tatooine\", \"41.9BBY\",  \"yes\"),  \n  c(\"Amidala\", \"female\", \"Naboo\", \"46BBY\", \"no\"),\n  c(\"Luke\", \"male\", \"Tatooine\", \"19BBY\", \"yes\"),\n  c(\"Leia\", \"female\", \"Alderaan\", \"19BBY\", \"no\")\n)\n\n# convert to data.frame and add column names\nswdf <- data.frame(sw_data, stringsAsFactors = FALSE)\nnames(swdf) <- c(\"Name\", \"Gender\", \"Homeworld\", \"Born\", \"Jedi\")\nswdf\n\n     Name Gender Homeworld    Born Jedi\n1  Anakin   male  Tatooine 41.9BBY  yes\n2 Amidala female     Naboo   46BBY   no\n3    Luke   male  Tatooine   19BBY  yes\n4    Leia female  Alderaan   19BBY   no\n\n\nThe default output when you pass a data frame to jsonlite::toJSON() is\n\n# convert R data.frame to JSON\nsw_json = toJSON(swdf)\nsw_json\n\n[{\"Name\":\"Anakin\",\"Gender\":\"male\",\"Homeworld\":\"Tatooine\",\"Born\":\"41.9BBY\",\"Jedi\":\"yes\"},{\"Name\":\"Amidala\",\"Gender\":\"female\",\"Homeworld\":\"Naboo\",\"Born\":\"46BBY\",\"Jedi\":\"no\"},{\"Name\":\"Luke\",\"Gender\":\"male\",\"Homeworld\":\"Tatooine\",\"Born\":\"19BBY\",\"Jedi\":\"yes\"},{\"Name\":\"Leia\",\"Gender\":\"female\",\"Homeworld\":\"Alderaan\",\"Born\":\"19BBY\",\"Jedi\":\"no\"}] \n\n\nThe argument dataframe gives you more control on the output. This argument has three options:\n\n\"rows\": each row is converted to a JSON-object with key-value pairs formed by \"column_name\": \"row_value\";\n\n\ntoJSON(swdf, dataframe = \"rows\")\n\n[{\"Name\":\"Anakin\",\"Gender\":\"male\",\"Homeworld\":\"Tatooine\",\"Born\":\"41.9BBY\",\"Jedi\":\"yes\"},{\"Name\":\"Amidala\",\"Gender\":\"female\",\"Homeworld\":\"Naboo\",\"Born\":\"46BBY\",\"Jedi\":\"no\"},{\"Name\":\"Luke\",\"Gender\":\"male\",\"Homeworld\":\"Tatooine\",\"Born\":\"19BBY\",\"Jedi\":\"yes\"},{\"Name\":\"Leia\",\"Gender\":\"female\",\"Homeworld\":\"Alderaan\",\"Born\":\"19BBY\",\"Jedi\":\"no\"}] \n\n\n\n\"columns\": each column is converted into a JSON-object with a single key for each column, and values stored as arrays;\n\n\ntoJSON(swdf, dataframe = \"columns\")\n\n{\"Name\":[\"Anakin\",\"Amidala\",\"Luke\",\"Leia\"],\"Gender\":[\"male\",\"female\",\"male\",\"female\"],\"Homeworld\":[\"Tatooine\",\"Naboo\",\"Tatooine\",\"Alderaan\"],\"Born\":[\"41.9BBY\",\"46BBY\",\"19BBY\",\"19BBY\"],\"Jedi\":[\"yes\",\"no\",\"yes\",\"no\"]} \n\n\n\n\"values\": the values in each column are converted to a JSON-array, and the names of the columns are lost.\n\n\ntoJSON(swdf, dataframe = \"values\")\n\n[[\"Anakin\",\"male\",\"Tatooine\",\"41.9BBY\",\"yes\"],[\"Amidala\",\"female\",\"Naboo\",\"46BBY\",\"no\"],[\"Luke\",\"male\",\"Tatooine\",\"19BBY\",\"yes\"],[\"Leia\",\"female\",\"Alderaan\",\"19BBY\",\"no\"]]"
  },
  {
    "objectID": "3-02-jsonlite-package.html#function-fromjson",
    "href": "3-02-jsonlite-package.html#function-fromjson",
    "title": "9  JSON R packages",
    "section": "9.2 Function fromJSON()",
    "text": "9.2 Function fromJSON()\nIn practice, instead of converting R objects to JSON objects, it is more common to have data in JSON format which needs to be converted into an R object.\nThe function jsonlite::fromJSON() converts a JSON-object to an R object.\n\nExample: JSON-array to R vector\n\njson_array <- '[\"computing\", \"with\", \"data\"]'\n\nfromJSON(json_array)\n\n[1] \"computing\" \"with\"      \"data\"     \n\n\n\n\nExample: JSON-object to R object\nConsider a simple JSON-object, and its conversion to R with jsonlite::fromJSON()\n\njson_obj1 <- '{\"name\": \"Jessica\"}'\n\nfromJSON(json_obj1)\n\n$name\n[1] \"Jessica\"\n\n\nNotice that the obtained object is an R list in which the key becomes the name of the list, and the value becomes the content of the list’s element.\nConsider a less simple JSON-object:\n\njson_obj2 <- '{\"name1\": \"Nicole\", \"name2\": \"Pleuni\", \"name3\": \"Rori\"}'\n\nfromJSON(json_obj2)\n\n$name1\n[1] \"Nicole\"\n\n$name2\n[1] \"Pleuni\"\n\n$name3\n[1] \"Rori\"\n\n\nAnother example:\n\nfromJSON('{\"name\": [\"X\", \"Y\"], \"grams\": [30, 20], \"qty\": [4, null],\n\"new\": [true, false]}')\n\n$name\n[1] \"X\" \"Y\"\n\n$grams\n[1] 30 20\n\n$qty\n[1]  4 NA\n\n$new\n[1]  TRUE FALSE\n\n\n\n\nExample: JSON-object to R object\nSuppose you have a JSON object with the following data:\n{\n  \"Name\": [\"Anakin\",\"Amidala\",\"Luke\",\"Leia\"],\n  \"Gender\": [\"male\",\"female\",\"male\",\"female\"],\n  \"Homeworld\": [\"Tatooine\",\"Naboo\",\"Tatooine\",\"Alderaan\"],\n  \"Born\": [\"41.9BBY\",\"46BBY\",\"19BBY\",\"19BBY\"],\n  \"Jedi\": [\"yes\",\"no\",\"yes\",\"no\"]\n}\nand assume that the above data is stored as a single (continuous) string in an R character vector json_sw; applying fromJSON() to this string gives you the following list:\n\n\n\n\nfromJSON(json_sw)\n\n$Name\n[1] \"Anakin\"  \"Amidala\" \"Luke\"    \"Leia\"   \n\n$Gender\n[1] \"male\"   \"female\" \"male\"   \"female\"\n\n$Homeworld\n[1] \"Tatooine\" \"Naboo\"    \"Tatooine\" \"Alderaan\"\n\n$Born\n[1] \"41.9BBY\" \"46BBY\"   \"19BBY\"   \"19BBY\"  \n\n$Jedi\n[1] \"yes\" \"no\"  \"yes\" \"no\" \n\n\nCan this be transformed into a data frame? Yes, by passing the obtained list to the function data.frame():\n\ndata.frame(fromJSON(json_sw))\n\n     Name Gender Homeworld    Born Jedi\n1  Anakin   male  Tatooine 41.9BBY  yes\n2 Amidala female     Naboo   46BBY   no\n3    Luke   male  Tatooine   19BBY  yes\n4    Leia female  Alderaan   19BBY   no"
  },
  {
    "objectID": "3-02-jsonlite-package.html#reading-json-data",
    "href": "3-02-jsonlite-package.html#reading-json-data",
    "title": "9  JSON R packages",
    "section": "9.3 Reading JSON Data",
    "text": "9.3 Reading JSON Data\nNow that we have discussed the basics of JSON, and the common ways to convert fromJSON() and toJSON(), let’s see how to read JSON data from the Web.\nOne of the typical ways to import JSON data from the Web to R is by passing the url directly to fromJSON(). Another way is by passing the name of the file with the JSON content as a single string to the function fromJSON().\nHere’s an example reading a JSON string from the website Advice Slip. The url https://api.adviceslip.com/advice gives you a random advice (see figure below):\n\n\n\n\n\nRandom advice from Advice Slip\n\n\n\n\nAs you can tell, the content is a simple JSON string\n\nadvice_url <- \"https://api.adviceslip.com/advice\"\n\nfromJSON(advice_url)\n\n\n\n$slip\n$slip$id\n[1] 9\n\n$slip$advice\n[1] \"True happiness always resides in the quest!\"\n\n\n\nExample: Colors in Hexadecimal Notation\nThe following data comes from one of Dave Eddy’s github repositories:\nhttps://raw.githubusercontent.com/bahamas10/css-color-names/master/css-color-names.json\nThis is a JSON-object in which the keys are color-names, and the values are the hexadecimal digits of the corresponding color:\n{\n  \"aliceblue\": \"#f0f8ff\",\n  \"antiquewhite\": \"#faebd7\",\n  \"aqua\": \"#00ffff\",\n  \"aquamarine\": \"#7fffd4\",\n  \"azure\": \"#f0ffff\",\n  ...\n  \"wheat\": \"#f5deb3\",\n  \"white\": \"#ffffff\",\n  \"whitesmoke\": \"#f5f5f5\",\n  \"yellow\": \"#ffff00\",\n  \"yellowgreen\": \"#9acd32\"\n}\n\n\n\nWe pass the url to jsonlite::fromJSON()\n\ncolors_json <- \"https://raw.githubusercontent.com/bahamas10/css-color-names/master/css-color-names.json\"\n\nhex_colors <- fromJSON(colors_json)\n\nThe output in hex_colors is a list with 148 elements; the first five elements are displayed below:\n\nhex_colors[1:5]\n\n$aliceblue\n[1] \"#f0f8ff\"\n\n$antiquewhite\n[1] \"#faebd7\"\n\n$aqua\n[1] \"#00ffff\"\n\n$aquamarine\n[1] \"#7fffd4\"\n\n$azure\n[1] \"#f0ffff\""
  },
  {
    "objectID": "4-01-apis-intro.html",
    "href": "4-01-apis-intro.html",
    "title": "10  Web APIs",
    "section": "",
    "text": "In this chapter we’ll give you a crash introduction to Web APIs, and how to use R for interacting with them.\nYou will need the following packages"
  },
  {
    "objectID": "4-01-apis-intro.html#introduction",
    "href": "4-01-apis-intro.html#introduction",
    "title": "10  Web APIs",
    "section": "10.1 Introduction",
    "text": "10.1 Introduction\nSo far we’ve been dealing with data sets in various formats: internal data objects in R (e.g. data tibble starwars), built-in data frames such as mtcars or oldfaithful), reading files stored in your computer (txt, csv, tsv, etc). But you also need to learn how to get data from the web.\nFor better or worse, reading data from the Web entails a whole other set of considerations. Because of the large variety of data formats available in the Web, we will primarily focus on retrieving data from Application Programming Interfaces also known as APIs.\nThe reason to focus on APIs is because nowadays many companies, websites, sources, etc. use APIs as their primary means to share information and data. Many large websites like Reddit, Twitter and Facebook offer APIs so that data analysts and data scientists can access interesting data. And having an API to share data has become a standard thing to have."
  },
  {
    "objectID": "4-01-apis-intro.html#a-little-bit-about-apis",
    "href": "4-01-apis-intro.html#a-little-bit-about-apis",
    "title": "10  Web APIs",
    "section": "10.2 A little bit about APIs",
    "text": "10.2 A little bit about APIs\nAPI stands for Application Programming Interface. If this sounds too fancy or cryptic for you, then simply think of it as a “Data Sharing Interface”.\nInstead of having to download a data file, an API allows programmers to request data directly from a website. From a technical point of view, an API is a set of rules, protocols, and tools for building software and applications.\n\nWhat is an API?\n“API” is a general term for the place where one computer program (the client) interacts with another (the server), or with itself.\nAPIs offer data scientists a polished way to request clean and curated data from a website. When a website like Facebook sets up an API, they are essentially setting up a computer that waits for data requests.\nOnce this computer receives a data request, it will do its own processing of the data and send it to the computer that requested it. From our perspective as the requester, we will need to write code in R that creates the request and tells the computer running the API what we need. That computer will then read our code, process the request, and return nicely-formatted data that can be easily parsed by existing R libraries.\n\n\nWhy to use an API?\nWhy is this valuable? Contrast the API approach to pure web scraping. When a programmer scrapes a web page, they receive the data in a messy chunk of HTML. While there are certainly libraries out there that make parsing HTML text easy, these are all cleaning steps that need to be taken before we even get our hands on the data we want!\nOften, we can immediately use the data we get from an API, which saves us time and frustration."
  },
  {
    "objectID": "4-01-apis-intro.html#using-r-as-an-http-client",
    "href": "4-01-apis-intro.html#using-r-as-an-http-client",
    "title": "10  Web APIs",
    "section": "10.3 Using R as an HTTP Client",
    "text": "10.3 Using R as an HTTP Client\nR has a few HTTP client packages: \"crul\", \"curl\", \"httr2\", and \"RCurl\"; you can think of them as “high-level R HTTP clients” which basically let you use R (and your computer) as an HTTP client.\nWe will describe how to use functions from \"httr2\" (pronounced hitter2)."
  },
  {
    "objectID": "4-01-apis-intro.html#interacting-with-aps-via-r",
    "href": "4-01-apis-intro.html#interacting-with-aps-via-r",
    "title": "10  Web APIs",
    "section": "10.4 Interacting with AP’s via R",
    "text": "10.4 Interacting with AP’s via R\nIn R, we can use the \"httr2\" package to make http requests and handle the responses.\nLet’s start with baby steps using the website https://api.adviceslip.com/ which provides an API—developed by Tom Kiss—to get a free piece of advice from the internet.\nThe first thing you need to do is to look at the adviceslip website to familiarize yourself with the functionalities it provides.\n\n\n\n\n\nAdvice Slip JSON API\n\n\n\n\n\nThe url https://api.adviceslip.com/advice will give you a random advice, for example:\n\n\n\n\n\nRandom advice from Advice Slip\n\n\n\n\nNotice that the format of the response is provided in JSON format. In the above example, the advice content is given as:\n{\"slip\": {\"id\": 9, \"advice\": \"True happiness always resides in the quest.\"}}\nwhich we can mentally rearrange as:\n{\n  \"slip\": {\n    \"id\": 9, \n    \"advice\": \"True happiness always resides in the quest.\"\n  }\n}\nGetting a random advice is quite simple, all you need is to make an HTTP request using the url https://api.adviceslip.com/advice.\n\n10.4.1 Making request from R\nInterestingly, we can make such request from R, using it as a server. This requires employing some functions from \"httr2\".\nIn \"httr2\", you start by creating a request. How? You use the request() function which creates a request object. To be clear, request() does not submit the request, it only creates the object, which you can use to build up a complex request piece by piece, and works well with the pipe operators |> or %>%.\n\n# start a request object\nadvice_url = \"https://api.adviceslip.com\"\nreq = request(advice_url)\nreq\n\n<httr2_request>\n\n\nGET https://api.adviceslip.com\n\n\nBody: empty\n\n\nTo see what this request will send to the server we perform a dry run:\n\nreq |> req_dry_run()\n\nGET / HTTP/1.1\nHost: api.adviceslip.com\nUser-Agent: httr2/1.0.0 r-curl/5.1.0 libcurl/7.79.1\nAccept: */*\nAccept-Encoding: deflate, gzip\n\n\nWhat’s going on with the first line GET / HTTP/1.1?\n\nThe first term, GET, refers to the HTTP method, which is a verb that tells the server what you want to do. In this case is GET, the most common verb, indicating that we want to get a resource. Other verbs include POST, to create a new resource; PUT, to replace an existing resource; and DELETE, to delete a resource.\nThe second part, /, is the path which is the URL stripped of details that the server already knows, i.e. the protocol (http or https), and the host (localhost).\nThe third element, HTTP/1.1, is the version of the HTTP protocol. This is unimportant for our purposes because it’s handled at a lower level.\n\nIn order to make a request with \"httr2\", we need to complete the full path of the URL. In the above example, this is done with req_url_path_append()\n\n# then we complete the full path\nreq |>\n  req_url_path_append(\"advice\") \n\n<httr2_request>\n\n\nGET https://api.adviceslip.com/advice\n\n\nBody: empty\n\n\nObserve that the output of this command has the full path of the request: https://api.adviceslip.com/advice.\n\n\n10.4.2 Performing a Request\nOnce we have the desired request object, then we can submit it or perform such request with req_perform():\n\n# then add on the query path\nresp = req |>\n  req_url_path_append(\"advice\") |>\n  req_perform()\n\nresp\n#> <httr2_response>\n#> GET https://api.adviceslip.com/advice\n#> Status: 200 OK\n#> Content-Type: text/html\n#> Body: In memory (77 bytes)\n\nAs you can tell, the response has a success status (200 OK), and the fetched content is text in html format.\nThe object resp is an object of class \"httr2_response\", which is basically an R list that contains 7 elements:\n\nnames(resp)\n#> [1] \"method\"      \"url\"         \"status_code\" \"headers\"     \"body\"       \n#> [6] \"request\"     \"cache\" \n\nOne of the elements in resp is \"body\" which is the element that is supposed to contain the response content. If we take a look at the body we get an interesting—but unhelpful—output:\n\nresp$body\n#>  [1] 7b 22 73 6c 69 70 22 3a 20 7b 20 22 69 64 22 3a 20 39 2c 20 22 61 64 76\n#> [25] 69 63 65 22 3a 20 22 54 72 75 65 20 68 61 70 70 69 6e 65 73 73 20 61 6c\n#> [49] 77 61 79 73 20 72 65 73 69 64 65 73 20 69 6e 20 74 68 65 20 71 75 65 73\n#> [73] 74 2e 22 7d 7d\n\nWhat kind of object is resp$body? Inspecting its class(), it turns out that this is an object of class \"raw\" or raw vector, which is a vector that holds raw bytes in R.\n\nclass(resp$body)\n#> [1] \"raw\"\n\nTechnically speaking, a raw vector is printed with each byte separately represented as a pair of hex digits. If you want to see a character representation (with escape sequences for non-printing characters) use rawToChar().\n\nbody_json = rawToChar(resp$body)\n\nbody_json\n#> [1] \"{\\\"slip\\\": { \\\"id\\\": 9, \\\"advice\\\": \\\"True happiness always resides in the quest.\\\"}}\"\n\nConverting the raw vector into text, we obtain the response body body_json which is text in JSON format. Then, to parse this JSON text, we use fromJSON() which returns an R list:\n\nslip_advice = fromJSON(body_json)\n\nslip_advice\n#> $slip\n#> $slip$id\n#> [1] 9\n\n#> $slip$advice\n#> [1] \"True happiness always resides in the quest.\"\n\nAnd finally, we extract the piece of advice as follows:\n\nslip_advice$slip$advice\n#> [1] \"True happiness always resides in the quest.\"\n\n\n\n10.4.3 Extracting response as string\nLet me show you another—more straightforward—way to extract the content of the response body by using one of the resp_body_() functions, in particular the resp_body_string() function:\n\nresp |> resp_body_string()\n#> [1] \"{\\\"slip\\\": { \\\"id\\\": 9, \\\"advice\\\": \\\"True happiness always resides in the quest.\\\"}}\"\n\nNotice that we obtain the same JSON text, which we can parse with fromJSON()\n\nresp |> resp_body_string() |> fromJSON()\n#> $slip\n#> $slip$id\n#> [1] 9\n\n#> $slip$advice\n#> [1] \"True happiness always resides in the quest.\"\n\n\n\n10.4.4 Extracting response as HTML\nA third equivalent method to extract the response body is with the function resp_body_html(). The reason why we can do that in this case has to do with the fact that the content of the response is in HTML format: Content-Type: text/html.\n\n# the result \"resp\" comes back as html\n# Let's extract body from response\ndoc_html = resp |> resp_body_html()\n\ndoc_html\n#> {html_document}\n#> <html>\n#> [1] <body><p>{\"slip\": { \"id\": 9, \"advice\": \"True happiness always resides ...\n\nSince the output is an html_document, we first need to extract the text of the <p> element. One option to achieve this is with an xpath expression:\n\ntxt_json = doc_html |> \n  xml_find_all(xpath = \"//p\") |>\n  xml_text()\n\ntxt_json\n#> [1] \"{\\\"slip\\\": { \\\"id\\\": 9, \\\"advice\\\": \\\"True happiness always resides in the quest.\\\"}}\"\n\nThis still requires some JSON parsing with fromJSON():\n\nslip_advice = fromJSON(txt_json)\nslip_advice\n#> $slip\n#> $slip$id\n#> [1] 9\n\n#> $slip$advice\n#> [1] \"True happiness always resides in the quest.\""
  },
  {
    "objectID": "4-01-apis-intro.html#example-search-with-advice-id",
    "href": "4-01-apis-intro.html#example-search-with-advice-id",
    "title": "10  Web APIs",
    "section": "10.5 Example: Search with Advice ID",
    "text": "10.5 Example: Search with Advice ID\nThe Advice Slip API also allows you to request an advice based on its ID.\n\n\n\n\n\nRandom advices by id\n\n\n\n\nFor example, the id = 5 results in the following advice:\n\n\n\n\n\nRandom advice from Advice Slip\n\n\n\n\nTo make this request from R, we create the request object, and then perform such request\n\n# advice id=5\nresp_advice_id5 <- req |>\n  req_url_path_append(\"advice/5\") |>\n  req_perform()\n\nresp_advice_id5\n#> <httr2_response>\n#> GET https://api.adviceslip.com/advice/5\n#> Status: 200 OK\n#> Content-Type: text/html\n#> Body: In memory (66 bytes)\n\nWe can then extract the response body, for instance:\n\nadvice_id5 = resp_advice_id5 |> resp_body_string() |> fromJSON()\nadvice_id5\n#> $slip\n#> $slip$id\n#> [1] 5\n\n#> $slip$advice\n#> [1] \"If you have the chance, take it!\""
  },
  {
    "objectID": "4-01-apis-intro.html#example-search-query",
    "href": "4-01-apis-intro.html#example-search-query",
    "title": "10  Web APIs",
    "section": "10.6 Example: Search Query",
    "text": "10.6 Example: Search Query\nAnother kind of request that you can do with the Advice Slip API is to search for an advice specifying a search query:\n\n\n\n\n\nRandom advices with search query\n\n\n\n\nFor example, say we are interested in searching for advice that includes the word chance. The corresponding URL path will have the following form:\nhttps://api.adviceslip.com/advice/search/chance\nAnd the resulting output looks like this:\n\n\n\n\n\nRandom advices with search query ‘chance’\n\n\n\n\nLet’s create a request that searches for the term chance. For sake of illustration, let’s build a request object by appending the path elements: advice, search, and chance\n\n# advice id=5\nresp_advice_chance <- req |>\n  req_url_path_append(\"advice\") |>\n  req_url_path_append(\"search\") |>\n  req_url_path_append(\"chance\") |>\n  req_perform()\n\nThe above command can be shorten as:\n\n# advice id=5\nresp_advice_chance <- req |>\n  req_url_path_append(\"advice/search/chance\") |>\n  req_perform()\n\nresp_advice_chance\n#> <httr2_response>\n#> GET https://api.adviceslip.com/advice/search/chance\n#> Status: 200 OK\n#> Content-Type: text/html\n#> Body: In memory (402 bytes)\n\nHaving performed the request, we fetch the data by extracting the body as a string (in JSON format), and then parsing it with fromJSON()\n\nadvice_chance = resp_advice_chance |> resp_body_string() |> fromJSON()\n\nnames(advice_chance)\n#> [1] \"total_results\" \"query\"         \"slips\" \n\nThe \"slips\" element contains a data frame with 3 advice recommendations (the last two are actually the same advice):\n\n\n\n\nadvice_chance$slips\n\n   id\n1   5\n2 184\n3 185\n                                                                                           advice\n1                                                                If you have the chance, take it!\n2 You can fail at what you don't want. So you might as well take a chance on doing what you love.\n3 You can fail at what you don't want. So you might as well take a chance on doing what you love.\n        date\n1 2016-12-25\n2 2017-03-10\n3 2016-08-05"
  },
  {
    "objectID": "4-02-api-open-notify.html",
    "href": "4-02-api-open-notify.html",
    "title": "11  Open Notify API",
    "section": "",
    "text": "In this chapter we provide another fairly simple example of an API: Open Notify.\nYou will need the following packages"
  },
  {
    "objectID": "4-02-api-open-notify.html#introduction",
    "href": "4-02-api-open-notify.html#introduction",
    "title": "11  Open Notify API",
    "section": "11.1 Introduction",
    "text": "11.1 Introduction\nOpen Notify is an open source project, developed and maintained by Nathan Bergey, to provide a simple application programming interface (API) for some of NASA’s data.\n\n\n\n\n\nOpen Notify API\n\n\n\n\n\nIf you visit Open Notify’s website https://www.open-notify.org, you’ll see that this API allows you to make 3 types of requests:\n\ncurrent location of the International Space Station (ISS)\noverhead pass predictions for the ISS\nnumber of astronauts in space"
  },
  {
    "objectID": "4-02-api-open-notify.html#current-iss-location",
    "href": "4-02-api-open-notify.html#current-iss-location",
    "title": "11  Open Notify API",
    "section": "11.2 Current ISS Location",
    "text": "11.2 Current ISS Location\nTo get the current location of the International Space Station (ISS) we need to check the documentation of the ISS Location Now webpage given in the link below:\nhttp://open-notify.org/Open-Notify-API/ISS-Location-Now/\n\n\n\n\n\nOpen Notify API\n\n\n\n\n\nInspecting the API documentation (see image below), it is possible to use Open Notify to obtain the current location of the ISS by making a request to http://api.open-notify.org/iss-now.json\n\n\n\n\n\nOpen Notify API\n\n\n\n\nTo reiterate, the URL to get the location data of the ISS is given by:\nhttp://api.open-notify.org/iss-now.json\nAt the time of this writing, the ISS position is:\n{\"iss_position\": {\"longitude\": \"-58.8531\", \"latitude\": \"-28.5444\"}, \"message\": \"success\", \"timestamp\": 1699732761}\nNotice that this output is given in JSON format which we can momentarily arrange as:\n{\n  \"iss_position\": {\n    \"longitude\": \"-58.8531\", \n    \"latitude\": \"-28.5444\"\n  }, \n  \"message\": \"success\", \n  \"timestamp\": 1699732761\n}\nWhat do we have? We get the longitude and latitude, the message status of the HTTP GET request, and also a UNIX timestamp expressed in number of seconds since Jan-01-1970.\n\n11.2.1 Getting Location (Approach 1)\nOne way to get the longitude and latitude coordinates of the ISS is by passing the URL directly to the fromJSON() function. Keep in mind that this method does not involve the explicit creation of an HTTP request.\n\niss_loc_url = \"http://api.open-notify.org/iss-now.json\"\n\niss_loc_list = fromJSON(iss_loc_json)\n\niss_loc_list\n#> $iss_position\n#> $iss_position$longitude\n#> [1] \"-58.8531\"\n#> \n#> $iss_position$latitude\n#> [1] \"-28.5444\"\n#> \n#> \n#> $message\n#> [1] \"success\"\n#> \n#> $timestamp\n#> [1] 1699732761\n\n\n\n\n\n\n11.2.2 Getting Location (Approach 2)\nAnother approach is to use functions from the R package \"httr2\".\nThe first step is to create a request object with request(), and then complete the full path by appending the JSON file name. After that, we call req_perform() to submit the request:\n\nreq = request(\"http://api.open-notify.org\")\n\n# then add on the query path\nresp = req |>\n  req_url_path_append(\"iss-now.json\") |>\n  req_perform()\n\nresp\n#> <httr2_response>\n#> GET http://api.open-notify.org/iss-now.json\n#> Status: 200 OK\n#> Content-Type: application/json\n#> Body: In memory (113 bytes)\n\nRecall that resp is basically an R list with the following elements:\n\nnames(resp)\n#> [1] \"method\"      \"url\"         \"status_code\" \"headers\"     \"body\"       \n#> [6] \"request\"     \"cache\" \n\nThe body element contains the content of the response, and it is in \"raw\" vector format by default:\n\nresp$body\n#>   [1] 7b 22 69 73 73 5f 70 6f 73 69 74 69 6f 6e 22 3a 20 7b 22 6c\n#>  [21] 61 74 69 74 75 64 65 22 3a 20 22 2d 31 32 2e 36 36 30 38 22\n#>  [41] 2c 20 22 6c 6f 6e 67 69 74 75 64 65 22 3a 20 22 2d 39 2e 35\n#>  [61] 30 37 33 22 7d 2c 20 22 6d 65 73 73 61 67 65 22 3a 20 22 73\n#>  [81] 75 63 63 65 73 73 22 2c 20 22 74 69 6d 65 73 74 61 6d 70 22\n#> [101] 3a 20 31 36 39 39 38 39 34 37 31 32 7d\n\nWe can use rawToChar() to convert the content into a JSON string:\n\nrawToChar(resp$body)\n#> [1] \"{\\\"iss_position\\\": {\\\"latitude\\\": \\\"-28.5444\\\", \\\"longitude\\\": \\\"-58.8531\\\"}, \\\"message\\\": \\\"success\\\", \\\"timestamp\\\": 1699732761}\"\n\nAlternatively, we can also use resp_body_json() to directly extract the JSON content as a list in R:\n\nresp |> resp_body_json()\n#> $iss_position\n#> $iss_position$longitude\n#> [1] \"-58.8531\"\n#> \n#> $iss_position$latitude\n#> [1] \"-28.5444\"\n#> \n#> $message\n#> [1] \"success\"\n#> \n#> $timestamp\n#> [1] 1699732761\n\n\n\n11.2.3 Map with \"leaflet\" (ver 1)\nWith the longitude and latitude of the ISS, we can make a map to visualize its location around the Earth. In an attempt to replicate the map displayed in Open Notify—which is based on a Leaflet map—we can try to get our own map.\nThe first step is to create a data.frame iss_loc_dat with the longitude (lng) and latitude (lat)\n\niss_loc_dat = data.frame(\n  lng = as.numeric(iss_loc_list$iss_position$longitude),\n  lat = as.numeric(iss_loc_list$iss_position$latitude)\n)\n\nWe pass this data to leaflet(), adding tiles, and a couple of markers to indicate the current location.\n\nleaflet(data = iss_loc_dat) |>\n  addTiles() |>\n  addCircleMarkers(\n    radius = 50,\n    stroke = FALSE, \n    fillOpacity = 0.3) |>\n  addMarkers() |>\n  setView(lng = iss_loc_dat$lng, lat = iss_loc_dat$lat, zoom = 3)\n\n\n\n\n\n\n\n11.2.4 Map with \"leaflet\" (ver 2)\nI was curious about the possibility of adding an image with the silhouette of the ISS. Luckily I found a PNG image file from wikimedia\nhttps://commons.wikimedia.org/wiki/File:International_Space_Station.svg\nThe image file that I chose, shown below, was the one with the smallest resolution: 320 x 202 pixels\n\n\n\n\n\nISS image from Wikimedia\n\n\n\n\n\n\n\n\nCreating an leaflet Icon\nWith the PNG file of the ISS, we have to create an icon via the \"leaflet\" function makeIcon(). For convenience, we need to resize the icon image by specifying iconWidth and iconHeight dimensions. Also, to center the icon image when displayed on the map, we also need to specify iconAnchorX and iconAnchorY values:\n\n# url of ISS icon from Wikimedia Commons (png file)\n# (broken into pieces so that it fits nicely on the screen)\nwikimedia = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/\"\niss = \"International_Space_Station.svg/\"\npng = \"320px-International_Space_Station.svg.png\"\niss_icon = paste0(wikimedia, iss, png)\n\n# define icon properties\nissIcon <- makeIcon(\n  iconUrl = iss_icon,\n  iconWidth = 100,\n  iconHeight = 70,\n  iconAnchorX = 50,\n  iconAnchorY = 35)\n\nThe last step involves making the map:\n\n# leaflet map\nleaflet(data = iss_loc_dat) |>\n  addTiles() |>\n  addCircleMarkers(\n    lng = ~lng, \n    lat = ~lat,\n    radius = 70,\n    stroke = FALSE, \n    fillOpacity = 0.3) |>\n  addMarkers(\n    lng = ~lng, \n    lat = ~lat, \n    icon = issIcon) |> \n  setView(lng = iss_loc_dat$lng, lat = iss_loc_dat$lat, zoom = 2)\n\n\n\n\n\nPretty nice, isn’t?"
  },
  {
    "objectID": "4-02-api-open-notify.html#astronauts-in-space",
    "href": "4-02-api-open-notify.html#astronauts-in-space",
    "title": "11  Open Notify API",
    "section": "11.3 Astronauts in Space",
    "text": "11.3 Astronauts in Space\nAnother kind of data that Open Notify lets you obtain is the number of astronauts in space. To get this data we need to check the documentation of the People in Space webpage given in the link below:\nhttp://open-notify.org/Open-Notify-API/People-In-Space/\n\n\n\n\n\nOpen Notify API: number of people in space\n\n\n\n\nTo reiterate, the URL to get the location data of the ISS is given by:\nhttp://api.open-notify.org/astros.json\nAt the time of this writing, the ISS position is:\n{\"message\": \"success\", \"people\": [{\"name\": \"Jasmin Moghbeli\", \"craft\": \"ISS\"}, {\"name\": \"Andreas Mogensen\", \"craft\": \"ISS\"}, {\"name\": \"Satoshi Furukawa\", \"craft\": \"ISS\"}, {\"name\": \"Konstantin Borisov\", \"craft\": \"ISS\"}, {\"name\": \"Oleg Kononenko\", \"craft\": \"ISS\"}, {\"name\": \"Nikolai Chub\", \"craft\": \"ISS\"}, {\"name\": \"Loral O'Hara\", \"craft\": \"ISS\"}], \"number\": 7}\nNotice that this output is given in JSON format which we can momentarily arrange as:\n{   \n  \"message\": \"success\", \n  \"people\": [\n    {\"name\": \"Jasmin Moghbeli\", \"craft\": \"ISS\"}, \n    {\"name\": \"Andreas Mogensen\", \"craft\": \"ISS\"}, \n    {\"name\": \"Satoshi Furukawa\", \"craft\": \"ISS\"}, \n    {\"name\": \"Konstantin Borisov\", \"craft\": \"ISS\"}, \n    {\"name\": \"Oleg Kononenko\", \"craft\": \"ISS\"}, \n    {\"name\": \"Nikolai Chub\", \"craft\": \"ISS\"}, \n    {\"name\": \"Loral O'Hara\", \"craft\": \"ISS\"}\n  ], \n  \"number\": 7\n}\nWhat do we have in this JSON object? We get the \"message\" status of the HTTP GET request, we also have an array \"people\" containing the data of the astronauts consisting of their \"name\" and \"craft\"; and finally we have the total \"number\" of people in space.\n\n11.3.1 Getting Data with HTTP request\nThe first step is to create a request object with request(), and then complete the full path by appending the JSON file name. After that, we call req_perform() to submit the request:\n\nreq = request(\"http://api.open-notify.org\")\n\n# then add on the query path\nresp = req |>\n  req_url_path_append(\"astros.json\") |>\n  req_perform()\n\nresp\n#> <httr2_response>\n#> GET http://api.open-notify.org/astros.json\n#> Status: 200 OK\n#> Content-Type: application/json\n#> Body: In memory (360 bytes)\n\nTo get the JSON content we use resp_body_json() which returns an R list:\n\nastros = resp |> resp_body_json()\n\nastros\n#> $message\n#> [1] \"success\"\n#> \n#> $people\n#> $people[[1]]\n#> $people[[1]]$name\n#> [1] \"Jasmin Moghbeli\"\n#> \n#> $people[[1]]$craft\n#> [1] \"ISS\"\n#> \n#> \n#> $people[[2]]\n#> $people[[2]]$name\n#> [1] \"Andreas Mogensen\"\n#> \n#> $people[[2]]$craft\n#> [1] \"ISS\"\n#> \n#> \n#> $people[[3]]\n#> $people[[3]]$name\n#> [1] \"Satoshi Furukawa\"\n#> \n#> $people[[3]]$craft\n#> [1] \"ISS\"\n#> \n#> \n#> $people[[4]]\n#> $people[[4]]$name\n#> [1] \"Konstantin Borisov\"\n#> \n#> $people[[4]]$craft\n#> [1] \"ISS\"\n#> \n#> \n#> $people[[5]]\n#> $people[[5]]$name\n#> [1] \"Oleg Kononenko\"\n#> \n#> $people[[5]]$craft\n#> [1] \"ISS\"\n#> \n#> \n#> $people[[6]]\n#> $people[[6]]$name\n#> [1] \"Nikolai Chub\"\n#> \n#> $people[[6]]$craft\n#> [1] \"ISS\"\n#> \n#> \n#> $people[[7]]\n#> $people[[7]]$name\n#> [1] \"Loral O'Hara\"\n#> \n#> $people[[7]]$craft\n#> [1] \"ISS\"\n#> \n#> \n#> \n#> $number\n#> [1] 7\n\nSo far, so good.\nBut what if we want to arrange the astronauts data into a table (e.g. data frame or tibble)? The astros object is a list, and the element $people—which is also a list—has the data we are interested in.\nThis is where we can use apply() functions such as lapply() and sapply(). Let’s start with lapply() by extracting the names of the astronauts which are in the first element of the list astros$people. Notice we do this by defining an anonymous function to get the precisely the first element of a list:\n\n# extract name of astronauts\nlapply(astros$people, function(x) x[[1]])\n#> [[1]]\n#> [1] \"Jasmin Moghbeli\"\n#> \n#> [[2]]\n#> [1] \"Andreas Mogensen\"\n#> \n#> [[3]]\n#> [1] \"Satoshi Furukawa\"\n#> \n#> [[4]]\n#> [1] \"Konstantin Borisov\"\n#> \n#> [[5]]\n#> [1] \"Oleg Kononenko\"\n#> \n#> [[6]]\n#> [1] \"Nikolai Chub\"\n#> \n#> [[7]]\n#> [1] \"Loral O'Hara\"\n\nThe issue is that we still have a list. The good news is that we can unlist() this output, ot we can also use sapply() instead of lapply() so that R returns a simplified output object, in this case a vector:\n\nastros_name = sapply(astros$people, function(x) x[[1]])\n\nastros_name\n#> [1] \"Jasmin Moghbeli\"    \"Andreas Mogensen\"   \"Satoshi Furukawa\"  \n#> [4] \"Konstantin Borisov\" \"Oleg Kononenko\"     \"Nikolai Chub\"      \n#> [7] \"Loral O'Hara\" \n\nTo get the name of the astronauts’ spacecrafts we repeat the same sapply() operation but now on the second element of the astros$people list\n\nastros_craft = sapply(astros$people, function(x) x[[2]])\n\nastros_craft\n#> [1] \"ISS\" \"ISS\" \"ISS\" \"ISS\" \"ISS\" \"ISS\" \"ISS\"\n\nAnd the last step involves building the table:\n\n\n\n\nastros_dat = data.frame(name = astros_name, craft = astros_craft)\nastros_dat\n\n                name craft\n1    Jasmin Moghbeli   ISS\n2   Andreas Mogensen   ISS\n3   Satoshi Furukawa   ISS\n4 Konstantin Borisov   ISS\n5     Oleg Kononenko   ISS\n6       Nikolai Chub   ISS\n7       Loral O'Hara   ISS"
  },
  {
    "objectID": "4-03-api-pubmed.html",
    "href": "4-03-api-pubmed.html",
    "title": "12  PubMed API Example",
    "section": "",
    "text": "In this chapter, we provide an example of web data collection from the database PubMed, using the Entrez Programming Utilities, commonly referred to as E-utilities, from the National Center for Biotechnology Information (NCBI).\nYou will need the following packages:"
  },
  {
    "objectID": "4-03-api-pubmed.html#pubmed",
    "href": "4-03-api-pubmed.html#pubmed",
    "title": "12  PubMed API Example",
    "section": "12.1 PubMed",
    "text": "12.1 PubMed\nPubMed (https://www.pubmed.gov) is a database of the largest collection of citations to medical journal literature in the world, and it is one of 38 databases built and maintained by the NCBI. Scientists, researchers, and users around the world use PubMed to search and retrieve bibliographic data, choose from several display formats, and share their results. Keep in mind that when people talk about PubMed, they could be referring to both the search interface and to the database itself.\nPubMed’s website provides a search engine to obtain bibliographic information:\n\n\n\n\n\nPartial screenshot of PubMed’s homepage\n\n\n\n\n\nThe simplest use of PubMed’s search engine is to provide a query, very similar to the queries that you would provide to google’s search engine. For example, we may be interested in looking for articles and other publications associated with some of the effects that exposure to Bisphenol A (BPA) has on reproductive health. Therefore, we can type in BPA exposure reproductive health inside the search box, and obtain some results (like those displayed in the screenshot below).\n\n\n\n\n\nPubMed search example\n\n\n\n\n\nAs of this writing (Fall 2020), there are 359 results that match the query term, with publications ranging from 1999 to end-of-2020. Notice that the webpage has a sidebar with checkboxes, and other intereactive options, that allow you to filter results by year of publication, by searching for the query in the abstract, or just in the titles, and things like that. As an example, we can move the slider for the year of publication to retrieve results that were published in 2018 and 2019 (see screenshot below)\n\n\n\n\n\nPubMed search example (cont’d)\n\n\n\n\n\nIf the obtained results are what you were looking for, you also have the option to download a CSV file with such results (download button in the navigation bar, above the barchart of years of publication).\nIn addition, you can perform a more advanced search by clicking on the “Advanced” button displayed below the search box. Clicking on this option will take you to a new page with more query boxes and a long list of query fields (screenshot below).\n\n\n\n\n\nPubMed search example (cont’d)\n\n\n\n\n\nIn the Advanced search mode, you can find a large list of fields that give you the opportunity to specify a more detailed query. For example, you can be more specific in your search by looking for results based on title, or based on author(s) information, or by date of publication, etc.\nThis way of interacting with PubMed (and similar databases) is how most users and researchers utilize PubMed: performing manual searches through their browsers, obtaining the results on their screens, and deciding which publications are worth further inspection. Sometimes, however, you may:\n\nhave a question that cannot be answered easily when you search PubMed,\nneed to see every citation in PubMed in a certain field,\nneed to run a search in PubMed and get the output in a CSV file that includes more (or different) data elements than the standard CSV file,\nneed to run specialized queries that might serve a very specific research need.\n\nThis is where E-utilities comes very handy because you can obtain the data that you need, and only the data that you need, in the format that you need. E-utilities is a great solution when you:\n\ncan’t find a good way to ask your question using the PubMed search box.\ncan search PubMed, but you’d like more, or less, or different data returned from the records.\nwant more control over the format of your PubMed data.\n\nE-utilities is simply another way to search PubMed and the other NCBI databases. Formally, E-utilities is an Application Programming Interface (API) which allows you to control exactly what fields you are searching, the specific data elements you retrieve, the format of the data, and how you share your results. When you use E-utilities to access PubMed, you are accessing the same data that you would find at https://www.pubmed.gov."
  },
  {
    "objectID": "4-03-api-pubmed.html#basics-of-e-utilities",
    "href": "4-03-api-pubmed.html#basics-of-e-utilities",
    "title": "12  PubMed API Example",
    "section": "12.2 Basics of E-utilities",
    "text": "12.2 Basics of E-utilities\nE-utilities is the short name for the Entrez Programming Utilities, which is simply another way to search PubMed and other NCBI databases. The E-utilities website is:\nhttps://www.ncbi.nlm.nih.gov/books/NBK25500/\nwhich contains the official documentation, written and maintained by Eric Sayers.\nAccording to the website:\n\n“Entrez Programming Utilities (E-utilities) are a set of nine server-side programs that provide a stable interface into the Entrez query and database system at the National Center for Biotechnology Information (NCBI).”\n\nWhat does this mean? E-utilities is basically an Application Programming Interface (API). The E-utilities API allows you to search PubMed and any other NCBI database through your own program: e.g. R, Python, etc. When you use E-utilities to access PubMed, you are accessing the same data that you’d find at https://www.pubmed.gov.\nNote: the results returned by E-utilities queries of PubMed may differ slightly from those returned in the web version of PubMed. As of this writing (Fall 2020) a new PubMed API is currently under development.\n\nThe Nine E-utilities\nThe name E-utilities comes from the nine utilities (or programs):\n\nEInfo (database statistics): Provides the number of records indexed in each field of a given database, the date of the last update of the database, and the available links from the database to other Entrez databases.\nESearch (database statistics): Responds to a text query with the list of matching UIDs in a given database (for later use in ESummary, EFetch or ELink), along with the term translations of the query.\nEPost (UID uploads): Accepts a list of UIDs from a given database, stores the set on the History Server, and responds with a query key and web environment for the uploaded dataset.\nESummary (document summary downloads): Responds to a list of UIDs from a given database with the corresponding document summaries.\nEFetch (data record downloads): Responds to a list of UIDs in a given database with the corresponding data records in a specified format.\nELink (Entrez links): Responds to a list of UIDs in a given database with either a list of related UIDs (and relevancy scores) in the same database or a list of linked UIDs in another Entrez database\nEGQuery (global query): Responds to a text query with the number of records matching the query in each Entrez database.\nESpell (spelling suggestions): Retrieves spelling suggestions for a text query in a given database.\nECitMatch (batch citation searching in PubMed): Retrieves PubMed IDs (PMIDs) corresponding to a set of input citation strings.\n\nFor illustration purposes, we will only focus on ESearch, ESummary, and EFetch.\n\n\n12.2.1 How does E-utilities work?\nThe way you use E-utilities is by assembling an e-utilities URL, following a specific set of rules, that you can use to make a request to one of its nine servers.\nBehind the scenes, the assembled E-utilities URL is translated into a standard set of input parameters that are used as the values necessary for various NCBI software components to search for and retrieve the requested data. In other words, the URLs direct requests to servers that are used only by the E-utilities and that are optimized to give users the best performance.\nBefore making any requests, keep in mind the following recommendation:\n\n“In order not to overload the E-utility servers, NCBI recommends that users post no more than three URL requests per second and limit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time during weekdays.”\n\nBy the way, you can obtain an API Key offering you enhanced levels of supported access to the E-utilities. This is totally optional but worth knowing. In our examples, we won’t need an API key, but if you plan to use PubMed more intensively then consider getting an API Key.\n\nAPI Requests\nTo make requests, you have to assemble a URL. Each URL consists of three parts:\n\nThe base URL: This is the address of the E-utilities server. Every URL begins with https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\nA utility name: This is the name of the specific tool that you are using. There are nine E-utilities and each one performs a specific function:\n\nESearch: Search a text query in a single database and retrieve the list of matching unique identifiers (UIDs). In PubMed, ESearch retrieves a list of PMIDs. esearch.fcgi?\nESummary: Retrieve document summaries for each UID. esummary.fcgi?\nEFetch: Retrieve full records for each UID. efetch.fcgi?\nEPost: Upload a list of UIDs for later use. epost.fcgi?\nELink: Retrieve UIDs for related or linked records, or LinkOut URLs. elink.fcgi?\nEInfo: Retrieve information and statistics about a single database. einfo.fcgi?\nESpell: Retrieve spelling suggestions for a text query. espell.fcgi?\nECitMatch: Search PubMed for a series of citation strings. ecitmatch.fcgi?\nEGQuery: Search a text query in all databases and return the number of results for the query in each database. egquery.fcgi?\n\n\nThe file extension .fcgi? stands for Fast Common Gateway Interface.\n\nThe parameters: These are the details of your query. Common parameters include the name of the database, your search terms, the number of results you would like to get, and the format of the output. The parameters that are available will change depending on the utility.\n\nWhen you put these three parts together, you will have a URL that looks something like this:\nhttps://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=reproductive+AND+health\nLet’s review three examples using utilities ESearch, ESummary, and EFetch.\n\n\nExample: Searching PubMed with ESearch\nSuppose we are interested in searching for bibliographic information in PubMed for the term reproductive health. Typically, the first thing to do when searching for information in PubMed is to retrieve a list of unique IDs for the documents that match the query text. All of this is done with ESearch, which means we need to use the esearch.fcgi? path to assemble the URL:\nhttps://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=reproductive+AND+health\nThe above url uses the ESearch utility (esearch.fcgi?) and uses two query parameters, namely db and term.\n\nhttps://eutils.ncbi.nlm.nih.gov/entrez/eutils/ is the base URL\nesearch.fcgi? indicates the E-utility (ESearch in this case)\ndb=pubmed: indicates that the searched database is PubMed.\nterm=reproductive+AND+health: indicates that the searched involves the term reproductive health.\nnotice the use of & to include a new query parameter\n\nBy default, the information is retrieved in XML format (see screenshot below). If you scroll through the XML output you’ll see the number of records retrieved along with a list of unique document PubMed IDs (PMIDs) for those records.\n\n\n\n\n\nList of PubMed IDs in XML format\n\n\n\n\n\n\nExample: Retrieving records with EFetch\nThe ESearch URL retrieves a list of PMIDs—not full records. To get the full records you need to use EFetch—providing the list of IDs—and the URL would look something like this:\nhttps://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=33073741,33073726&retmode=abstract&rettype=text\n\nhttps://eutils.ncbi.nlm.nih.gov/entrez/eutils/ is the base URL\nefetch.fcgi? indicates the E-utility (EFectch in this case)\ndb=pubmed: indicates that the searched database is PubMed.\nid=33073741,33073726: includes the PMIDs of the records.\nretmode=abstract: indicates the abstract as the return mode.\nrettype=text: determines the text as the return type format.\n\n\n\n\n\n\nReturn type in text format for abstract of specified IDs\n\n\n\n\n\n\nExample: Retrieving summaries with ESummary\nThe ESummary URL allows you to get the document summaries of the associated list of PMIDs, and the URL would look something like this:\nhttps://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id=33073741,33073726\n\nhttps://eutils.ncbi.nlm.nih.gov/entrez/eutils/ is the base URL\nesummary.fcgi? indicates the E-utility (ESummary in this case)\ndb=pubmed: indicates that the searched database is PubMed.\nid=33073741,33073726: includes the PMIDs of the records.\n\nThe output of ESummary is a series of XML \"DocSums\" (Document Summaries), the format of which depends on the database. Below is an example DocSum for PubMed ID 33073741.\n\n\n\n\n\nSummary in XML format for specified IDs\n\n\n\n\n\n\n\n12.2.2 Searching PubMed from within R\nNow that you have a basic understanding of PubMed, and the E-Utilities programs, let’s see how to make requests from R.\nWe are going to consider a typical pipeline that involves three steps:\n\nuse ESearch to get a list of document IDs (default output in XML)\nparse the IDs from the XML document\npass the list of IDs to either EFetch or ESummary\n\nStep 1: As a first step, we can define a character string entrez_url with the base URL, and three more strings for each of the E-utilities: esearch, efetch, and esummary:\n\n# base URL, and paths of associated e-utilities\nentrez_url <- \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n\nesearch <- \"esearch.fcgi?\"\nefetch <- \"efetch.fcgi?\"\nesummary <- \"esummary.fcgi?\"\n\nSuppose we are interested in performing a search with the following parameters:\n\nsearching for the term BPA exposure reproductive health;\nlimiting our search to documents between 2018 and 2019\nretaining at most the first 100 results\n\nStep 2: We can assemble an R string params_esearch with all the previous ESearch query parameters:\n\n# assembling string of parameters for the query\nparams_esearch <- paste0(\n  c(\"db=pubmed\", \n    \"term=BPA+exposure+reproductive+health\",\n    \"mindate=2018\",\n    \"maxdate=2019\",\n    \"retmax=100\"),\n  collapse = \"&\")\n\nparams_esearch\n\n[1] \"db=pubmed&term=BPA+exposure+reproductive+health&mindate=2018&maxdate=2019&retmax=100\"\n\n\nStep 3: With the base URL, the esearch utility, and the parameters, we assemble the required URL that we can pass to read_xml(), and then use html_nodes() and html_text() to extract the list of documents IDs:\n\n\n\n\n# assemble an esearch URL\nids_query <- paste0(entrez_url, esearch, params_esearch)\n\n# Retrieving IDs with ESearch\nids_xml <- read_xml(ids_query)\n\n# extract Ids (PMIDs)\nids <- html_text(html_nodes(ids_xml, xpath = \"//Id\"))\n\nThe vector ids contains 94 retrieved document IDs, the first 5 and the last 5 five displayed below:\n\nhead(ids, 5)\n\n[1] \"31771501\" \"31697385\" \"31683046\" \"31658598\" \"31648075\"\n\ntail(ids, 5)\n\n[1] \"29549734\" \"29428396\" \"29415642\" \"29385186\" \"29317319\"\n\n\nStep 4: We can then use this list of IDs to assemble a URL for obtaining summary information of documents—via ESummary. Notice that Esummary uses different parameters from the ones used in ESearch. The query parameters in this case are db (name of database) and id (list of IDs). With the base URL, the esummary utility, and the parameters params_esummary, we assemble the required URL that we can pass to read_xml():\n\n# parameters for esummary\nparams_esummary <- paste0(\n  \"db=pubmed&id=\", \n  paste(ids, collapse = \",\"))\n\n# assemble an esummary URL\nsummaries_query <- paste0(entrez_url, esummary, params_esummary)\n\nsummaries_xml <- read_xml(summaries_query)\n\nStep 5: To inspect the XML output we can use functions from package \"xml2\" such as xml_child() for instance. Let’s take a look at the first node:\n\n\n\n\nxml_child(summaries_xml, search = 1)\n\n{xml_node}\n<DocSum>\n [1] <Id>31771501</Id>\n [2] <Item Name=\"PubDate\" Type=\"Date\">2019 Oct</Item>\n [3] <Item Name=\"EPubDate\" Type=\"Date\"/>\n [4] <Item Name=\"Source\" Type=\"String\">Toxicol Ind Health</Item>\n [5] <Item Name=\"AuthorList\" Type=\"List\">\\n  <Item Name=\"Author\" Type=\"String ...\n [6] <Item Name=\"LastAuthor\" Type=\"String\">Sun Z</Item>\n [7] <Item Name=\"Title\" Type=\"String\">Oral exposure to low-dose bisphenol A i ...\n [8] <Item Name=\"Volume\" Type=\"String\">35</Item>\n [9] <Item Name=\"Issue\" Type=\"String\">10</Item>\n[10] <Item Name=\"Pages\" Type=\"String\">647-659</Item>\n[11] <Item Name=\"LangList\" Type=\"List\">\\n  <Item Name=\"Lang\" Type=\"String\">En ...\n[12] <Item Name=\"NlmUniqueID\" Type=\"String\">8602702</Item>\n[13] <Item Name=\"ISSN\" Type=\"String\">0748-2337</Item>\n[14] <Item Name=\"ESSN\" Type=\"String\">1477-0393</Item>\n[15] <Item Name=\"PubTypeList\" Type=\"List\">\\n  <Item Name=\"PubType\" Type=\"Stri ...\n[16] <Item Name=\"RecordStatus\" Type=\"String\">PubMed - indexed for MEDLINE</Item>\n[17] <Item Name=\"PubStatus\" Type=\"String\">ppublish</Item>\n[18] <Item Name=\"ArticleIds\" Type=\"List\">\\n  <Item Name=\"pubmed\" Type=\"String ...\n[19] <Item Name=\"DOI\" Type=\"String\">10.1177/0748233719885565</Item>\n[20] <Item Name=\"History\" Type=\"List\">\\n  <Item Name=\"entrez\" Type=\"Date\">201 ...\n...\n\n\nThe obtained summary information contains various fields such as date of publication PubDate, list of authors AuthorList, title Title, starting and ending pages Pages, etc.\nStep 6: To extract all the titles, we look for the Item nodes with argument Name=\"Title\". To be more precise, we can define an XPath pattern '//Item[@Name=\"Title\"]' for xml_nodes(), and then extract the content with xml_text():\n\ntitle_nodes <- xml_nodes(summaries_xml, xpath = '//Item[@Name=\"Title\"]')\n\nWarning: `xml_nodes()` was deprecated in rvest 1.0.0.\nℹ Please use `html_elements()` instead.\n\ntitles <- xml_text(title_nodes)\nhead(titles)\n\n[1] \"Oral exposure to low-dose bisphenol A induces hyperplasia of dorsolateral prostate and upregulates EGFR expression in adult <i>Sprague-Dawley</i> rats.\"                                   \n[2] \"Juvenile Toxicity Rodent Model to Study Toxicological Effects of Bisphenol A (BPA) at Dose Levels Derived From Italian Children Biomonitoring Study.\"                                      \n[3] \"Bisphenol F exposure impairs neurodevelopment in zebrafish larvae (Danio rerio).\"                                                                                                          \n[4] \"Effects of Dietary Bisphenol A on the Reproductive Function of Gilthead Sea Bream (<i>Sparus aurata</i>) Testes.\"                                                                          \n[5] \"Urinary bisphenol A concentration is correlated with poorer oocyte retrieval and embryo implantation outcomes in patients with tubal factor infertility undergoing in vitro fertilisation.\"\n[6] \"In utero exposure to persistent and nonpersistent endocrine-disrupting chemicals and anogenital distance. A systematic review of epidemiological studies†.\"                                \n\n\nFinally, we can make a wordcloud. The code below has a series of commands to perform some text processing (with functions from the text mining package \"tm\") with the main terms or words used to create the wordcloud.\n\n# title words\ntwords <- unlist(str_split(titles, \" \"))\ntwords <- tolower(twords)\n\n# auxiliary vector of stop words\nstop_words <- c(\n  stopwords(kind = \"en\"),\n  c(\"bisphenol\", \"exposure\", \"reproductive\", \"health\", \"bpa\")\n)\n\n# removing unnecessary things\ntwords <- removePunctuation(twords)\ntwords <- removeNumbers(twords)\ntwords <- removeWords(twords, stopwords(kind = \"en\"))\ntwords <- removeWords(twords, stop_words)\ntwords <- twords[twords != \"\"]\n\n# data frame of terms and frequencies\ndat_words <- as.data.frame(table(twords))\ndat_words <- dat_words[order(dat_words$Freq, decreasing = TRUE), ]\nhead(dat_words, n = 10)\n\n           twords Freq\n161       effects   18\n500         study   10\n519      toxicity    9\n313          male    8\n192             f    7\n197        female    7\n303        levels    7\n464             s    7\n173 environmental    6\n180      estrogen    6\n\n\nand then plotting a wordcloud with the function wordcloud2() from the package \"wordcloud2\":\n\nwordcloud2(dat_words)"
  }
]